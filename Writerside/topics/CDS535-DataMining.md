# CDS535 DataMining
- The exam has nine Questions
- Data contains value and knowledge

## 1. What is Data Mining?

### 1. Definition of Data Mining
- Non-trivial extraction of implicit, previously unknown and potentially useful information from data
- Exploration & analysis, by automatic or semi-automatic means, of large quantities of data to discover meaningful patterns
- ![image.png](image.png)

### 2. Data Science compared to Data Mining
**Business Understanding**
- Seldom pre-packaged as clear data science problems.
- Creative problem formulation is needed to consider the business problem as one or more data science problems.
- High-level knowledge of the domains helps creative Data
- Scientists/Business Analysts see novel formulations.
- The design team should think carefully about the problem to be solved and about the user scenario.

**Data Understanding**
- Understand the strengths and limitations of the data.
- Historical data often are collected for other purposes unrelated to the current data science problem.

**Data Preparation**
- Many data science/data mining algorithms require data to be in a specific form (e.g., Table format).
- Some conversion will be necessary.
- Missing values should be handled.
- 
**Modeling**
- Apply data science/data mining algorithms to create models/data science results from the data

**Evaluation**
- Assess the data science results.
- Gain confidence that they are correct and reliable.
- Test the results (models) in a controlled laboratory setting.
- Ensure that the model satisfies the original business goals.

**Deployment**
- The data science results (models) are used in real problems (e.g., fake detections).
- Implement it in some information system or business process.
- Deploying a model into a production system typically requires that the model be re-coded for the production environment (e.g., program in Windows environment).

![image_2.png](image_2.png)

### 3. Data Mining Process
![image_3.png](image_3.png)

## 2. Data Mining Tasks
**Prediction Methods**
- Use some variables to predict unknown or future values of other variables.

**Description Methods**
- Find human-interpretable patterns that describe the data.

äºŒã€è¿‡ç¨‹ä¸æ­¥éª¤
- Prediction Methodsï¼ˆé¢„æµ‹æ–¹æ³•ï¼‰
- æ•°æ®æ”¶é›†ï¼šæ”¶é›†ä¸é¢„æµ‹ç›®æ ‡ç›¸å…³çš„å†å²æ•°æ®ã€‚ 
- æ•°æ®é¢„å¤„ç†ï¼šæ¸…æ´—æ•°æ®ã€å¤„ç†ç¼ºå¤±å€¼ã€æ ‡å‡†åŒ–ç­‰ã€‚ 
- ç‰¹å¾é€‰æ‹©ï¼šé€‰æ‹©å¯¹é¢„æµ‹ç›®æ ‡æœ‰å½±å“çš„ç‰¹å¾ã€‚ 
- æ¨¡å‹è®­ç»ƒï¼šä½¿ç”¨è®­ç»ƒæ•°æ®é›†è®­ç»ƒé¢„æµ‹æ¨¡å‹ã€‚ 
- æ¨¡å‹éªŒè¯ï¼šä½¿ç”¨éªŒè¯æ•°æ®é›†éªŒè¯æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚ 
- é¢„æµ‹ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æœªçŸ¥æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚


- Description Methodsï¼ˆæè¿°æ–¹æ³•ï¼‰ 
- æ•°æ®æ”¶é›†ï¼šæ”¶é›†ä¸ç›®æ ‡é—®é¢˜ç›¸å…³çš„æ•°æ®ã€‚ 
- æ•°æ®æ¢ç´¢ï¼šé€šè¿‡å¯è§†åŒ–ã€ç»Ÿè®¡ç­‰æ–¹æ³•åˆæ­¥äº†è§£æ•°æ®ã€‚ 
- æ¨¡å¼å‘ç°ï¼šè¿ç”¨æ•°æ®æŒ–æ˜æŠ€æœ¯å‘ç°æ•°æ®ä¸­çš„æ¨¡å¼ã€å…³è”å’Œå¼‚å¸¸ç­‰ã€‚ 
- æ¨¡å¼è§£é‡Šï¼šä»¥äººç±»å¯ç†è§£çš„æ–¹å¼è§£é‡Šå‘ç°çš„æ¨¡å¼ã€‚ 
- ç»“æœå‘ˆç°ï¼šé€šè¿‡å›¾è¡¨ã€æŠ¥å‘Šç­‰å½¢å¼å‘ˆç°æè¿°ç»“æœã€‚

ä¸‰ã€åº”ç”¨å®ä¾‹
- Prediction Methodsï¼ˆé¢„æµ‹æ–¹æ³•ï¼‰ 
- é‡‘èé¢†åŸŸï¼šé¢„æµ‹è‚¡ç¥¨ä»·æ ¼ã€æ±‡ç‡ã€ä¿¡ç”¨é£é™©ç­‰ã€‚ 
- é”€å”®é¢†åŸŸï¼šé¢„æµ‹é”€å”®é¢ã€å¸‚åœºä»½é¢ã€å®¢æˆ·è¡Œä¸ºç­‰ã€‚ 
- åŒ»ç–—é¢†åŸŸï¼šé¢„æµ‹ç–¾ç—…å‘ç”Ÿç‡ã€æ²»ç–—æ•ˆæœã€æ‚£è€…é¢„åç­‰ã€‚

- Description Methodsï¼ˆæè¿°æ–¹æ³•ï¼‰ 
- å¸‚åœºç ”ç©¶ï¼šæè¿°æ¶ˆè´¹è€…è¡Œä¸ºã€å¸‚åœºè¶‹åŠ¿ç­‰ã€‚ 
- ç¤¾ä¼šç§‘å­¦ï¼šæè¿°ç¤¾ä¼šç°è±¡ã€äººç±»è¡Œä¸ºç­‰ã€‚ 
- ç”Ÿç‰©ä¿¡æ¯å­¦ï¼šæè¿°åŸºå› è¡¨è¾¾ã€è›‹ç™½è´¨ç»“æ„ç­‰ã€‚

### Data Mining Tasks
- Classification [Predictive]
- Clustering [Descriptive]
- Association Rule Discovery [Descriptive]  
- Sequential Pattern Discovery [Descriptive]  
- Regression [Predictive]
- Deviation Detection [Predictive]

![image_4.png](image_4.png)

## 3. Predictive Modeling

### 1. predictive modeling: classification
- Find a model	for class attribute as a function of the values of other attributes
- <note>æœ¬ç« å…·ä½“å†…å®¹è§ä»¥ä¸‹é“¾æ¥</note>
- [](#7-classification)

### 2. predictive modeling: regression
- Predict a value of a given continuous valued variable based on the values of other variables, assuming a linear or nonlinear model of dependency. 
- Extensively studied in statistics, neural network fields.

### 3. predictive modeling: clustering
- Finding groups of objects such that the objects in a
group will be similar (or related) to one another and
different from (or unrelated to) the objects in other
groups

- What is not Cluster Analysis?
  1. Supervised classification
  2. Simple segmentation
  3. Results of a query
  4. Graph partitioning
  <tip>èšç±»åˆ†ææ˜¯ä¸€ç§æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå®ƒåŸºäºå¯¹è±¡çš„å›ºæœ‰ç›¸ä¼¼æ€§æˆ–è·ç¦»æ¥è¿›è¡Œåˆ†ç»„ï¼Œè€Œä¸ä½¿ç”¨å¸¦æœ‰æ ‡ç­¾çš„æ•°æ®æˆ–å¤–éƒ¨è§„èŒƒã€‚</tip>

#### 1. åˆ’åˆ†èšç±»ï¼ˆPartitional Clusteringï¼‰
- A division of data objects into non-overlapping subsets (clusters)
- åˆ’åˆ†èšç±»æ˜¯å°†æ•°æ®å¯¹è±¡åˆ†æˆéé‡å çš„å­é›†ï¼ˆå³èšç±»ï¼‰ï¼Œæ¯ä¸ªæ•°æ®ç‚¹åªèƒ½å±äºä¸€ä¸ªèšç±»ã€‚è¿™ç§æ–¹æ³•é€šå¸¸éœ€è¦é¢„å…ˆæŒ‡å®šèšç±»çš„æ•°é‡Kï¼Œç„¶åç®—æ³•ä¼šå°è¯•æ‰¾åˆ°Kä¸ªèšç±»ï¼Œä½¿å¾—æ¯ä¸ªæ•°æ®ç‚¹éƒ½è¢«åˆ†é…åˆ°ä¸€ä¸ªèšç±»ä¸­ï¼Œä¸”æ¯ä¸ªèšç±»ä¸­çš„æ•°æ®ç‚¹å°½å¯èƒ½ç›¸ä¼¼ã€‚
åˆ’åˆ†èšç±»çš„å…¸å‹ç®—æ³•åŒ…æ‹¬K-meanså’ŒK-ä¸­å¿ƒç‚¹ç®—æ³•ç­‰ã€‚è¿™äº›ç®—æ³•é€šè¿‡è¿­ä»£çš„æ–¹å¼ä¸æ–­ä¼˜åŒ–èšç±»çš„ç»“æœï¼Œç›´åˆ°è¾¾åˆ°æŸä¸ªç»ˆæ­¢æ¡ä»¶ï¼ˆå¦‚èšç±»ä¸­å¿ƒä¸å†å‘ç”Ÿå˜åŒ–æˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼‰ã€‚

<p style="display: block;">
  <img src="image_152.png" alt="image_152"/>
</p>
<note>åœ¨åˆ’åˆ†èšç±»ä¸­ï¼Œæœ€å¸¸ç”¨çš„ç®—æ³•ä¹‹ä¸€æ˜¯K-meansç®—æ³•ã€‚</note>

##### 1. K-means Clustering
- Partitional clustering approach
- Number of clusters, K, must be specified
- Each cluster is associated with a centroid (center point)
- Each point is assigned to the cluster with the closest
  centroid
- The basic algorithm is basic

<p style="display: block;">
    <img src="image_5.png" alt="image_5"/>
</p>

- Steps

1.  é€‰æ‹©åˆå§‹è´¨å¿ƒ
    - è¿™ä¸€æ­¥é€šå¸¸éšæœºä»æ•°æ®é›†ä¸­é€‰æ‹©Kä¸ªç‚¹ä½œä¸ºåˆå§‹è´¨å¿ƒã€‚è™½ç„¶éšæœºé€‰æ‹©ç®€å•æ˜“è¡Œï¼Œä½†å®ƒå¯èƒ½å¯¼è‡´ç®—æ³•æ”¶æ•›åˆ°ä¸åŒçš„å±€éƒ¨æœ€ä¼˜è§£ï¼Œå› ä¸ºæ¯æ¬¡è¿è¡Œç®—æ³•æ—¶ï¼Œåˆå§‹è´¨å¿ƒçš„é€‰æ‹©éƒ½å¯èƒ½ä¸åŒã€‚
2.  è¿­ä»£è¿‡ç¨‹
    1. **åˆ†é…æ•°æ®ç‚¹åˆ°æœ€è¿‘è´¨å¿ƒ**ï¼šå¯¹äºæ•°æ®é›†ä¸­çš„æ¯ä¸ªç‚¹ï¼Œè®¡ç®—å…¶ä¸æ‰€æœ‰è´¨å¿ƒçš„è·ç¦»ï¼ˆå¦‚æ¬§å‡ é‡Œå¾—è·ç¦»ï¼‰ï¼Œå¹¶å°†å…¶åˆ†é…ç»™æœ€è¿‘çš„è´¨å¿ƒï¼Œä»è€Œå½¢æˆä¸€ä¸ªèšç±»ã€‚
    2. **é‡æ–°è®¡ç®—è´¨å¿ƒ**ï¼šå¯¹äºæ¯ä¸ªèšç±»ï¼Œè®¡ç®—å…¶æ‰€æœ‰ç‚¹çš„å¹³å‡å€¼ï¼ˆæˆ–å…¶ä»–ä¸­å¿ƒè¶‹åŠ¿çš„åº¦é‡ï¼‰ï¼Œå¹¶å°†è¿™ä¸ªæ–°çš„å¹³å‡å€¼ä½œä¸ºè¯¥èšç±»çš„è´¨å¿ƒã€‚
    3. **é‡å¤ä¸Šè¿°ä¸¤ä¸ªæ­¥éª¤**ï¼Œç›´åˆ°è´¨å¿ƒçš„ä½ç½®ä¸å†å‘ç”Ÿæ˜¾è‘—å˜åŒ–ï¼ˆå³ç®—æ³•æ”¶æ•›ï¼‰ï¼Œæˆ–è€…è¾¾åˆ°é¢„è®¾çš„è¿­ä»£æ¬¡æ•°ã€‚

3.  æ”¶æ•›æ¡ä»¶
    - å¸¸è§çš„æ”¶æ•›æ¡ä»¶æ˜¯è´¨å¿ƒåœæ­¢å˜åŒ–ï¼Œä½†å®é™…åº”ç”¨ä¸­ï¼Œä¹Ÿå¯ä»¥é‡‡ç”¨å…¶ä»–æ¡ä»¶ï¼Œå¦‚â€œç›´åˆ°ç›¸å¯¹è¾ƒå°‘çš„æ•°æ®ç‚¹æ”¹å˜å…¶æ‰€å±èšç±»â€ã€‚è¿™æ˜¯å› ä¸ºï¼Œåœ¨ç®—æ³•çš„åæœŸé˜¶æ®µï¼Œéšç€è´¨å¿ƒçš„é€æ¸ç¨³å®šï¼Œå¾ˆå°‘æœ‰æ•°æ®ç‚¹ä¼šæ”¹å˜å…¶èšç±»å½’å±ã€‚

4.  ç®—æ³•å¤æ‚åº¦
    - K-meansèšç±»ç®—æ³•çš„å¤æ‚åº¦ä¸º O(n * K * I * d)ï¼Œå…¶ä¸­ n æ˜¯æ•°æ®ç‚¹çš„æ•°é‡ï¼ŒK æ˜¯èšç±»çš„æ•°é‡ï¼ŒI æ˜¯è¿­ä»£æ¬¡æ•°ï¼Œd æ˜¯æ•°æ®ç‚¹çš„å±æ€§æ•°é‡ã€‚è¿™ä¸ªå¤æ‚åº¦è¡¨æ˜ï¼Œç®—æ³•çš„è®¡ç®—æˆæœ¬éšç€æ•°æ®é›†çš„å¤§å°ã€èšç±»æ•°é‡å’Œè¿­ä»£æ¬¡æ•°çš„å¢åŠ è€Œå¢åŠ ã€‚

5.  è´¨å¿ƒçš„å®šä¹‰
    - åœ¨ K-means èšç±»ä¸­ï¼Œè´¨å¿ƒé€šå¸¸æ˜¯èšç±»ä¸­æ‰€æœ‰ç‚¹çš„å¹³å‡å€¼ã€‚ç„¶è€Œï¼Œä¹Ÿå¯ä»¥é‡‡ç”¨å…¶ä»–å®šä¹‰ï¼Œå¦‚ä¸­ä½æ•°æˆ–åŠ æƒå¹³å‡å€¼ï¼Œä»¥é€‚åº”ä¸åŒçš„æ•°æ®é›†å’Œèšç±»éœ€æ±‚ã€‚

6.  æ”¶æ•›æ€§
    - å¯¹äºå¸¸è§çš„è·ç¦»åº¦é‡ï¼ˆå¦‚æ¬§å‡ é‡Œå¾—è·ç¦»ï¼‰å’Œé€‚å½“å®šä¹‰çš„è´¨å¿ƒï¼ŒK-meansç®—æ³•å°†æ”¶æ•›ã€‚ç„¶è€Œï¼Œç”±äºåˆå§‹è´¨å¿ƒçš„é€‰æ‹©æ˜¯éšæœºçš„ï¼Œå› æ­¤ç®—æ³•å¯èƒ½æ”¶æ•›åˆ°ä¸åŒçš„å±€éƒ¨æœ€ä¼˜è§£ã€‚

7.  è¿­ä»£æ¬¡æ•°
    - åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œç®—æ³•çš„å¤§éƒ¨åˆ†æ”¶æ•›å‘ç”Ÿåœ¨å‰å‡ æ¬¡è¿­ä»£ä¸­ã€‚éšç€è¿­ä»£çš„è¿›è¡Œï¼Œè´¨å¿ƒçš„å˜åŒ–é€æ¸å‡å°ï¼Œç›´åˆ°è¾¾åˆ°æ”¶æ•›æ¡ä»¶ã€‚


- Simple iterative algorithm.
  - Choose initial centroids;
  - repeat {assign each point to a nearest centroid; re-compute cluster centroids}
  - until centroids stop changing.
- Initial centroids are often chosen randomly.
  -  Clusters produced can vary from one run to another
- The centroid is (typically) the mean of the points in the cluster,
  but other definitions are possible.
- K-means will converge for common proximity measures (e.g., Euclidean Distance, p.62) with appropriately defined centroid
- Most of the convergence happens in the first few iterations.
  - Often the stopping condition is changed to â€˜Until relatively few points change clustersâ€™
- Complexity is O( n * K * I * d)
  - n = number of points
  - K = number of clusters
  - I = number of iterations
  - d = number of attributes

- K-means Objective Function
  - A common objective function (used with Euclidean distance measure) is Sum of Squared Error (SSE)
    - For each point, the error is the distance to the nearest cluster center
    - To get SSE, we square these errors and sum them.
    $$
    \text{SSE} = \sum_{i=1}^{K} \sum_{x \in C_i} \text{dist}^2(m_i, x)
    $$
    - x is a data point in cluster Ci and mi is the centroid (mean) for
cluster Ci
    - SSE improves in each iteration of K-means until it reaches a local or global minima.
    
- è¯¯å·®å¹³æ–¹å’Œï¼ˆSSEï¼‰
  - åœ¨K-meansèšç±»ç®—æ³•ä¸­ï¼ŒSSEç”¨äºé‡åŒ–æ•°æ®ç‚¹åˆ°å…¶æ‰€å±èšç±»ä¸­å¿ƒçš„è·ç¦»ä¹‹å’Œã€‚å¯¹äºæ¯ä¸ªæ•°æ®ç‚¹ï¼Œå…¶è¯¯å·®å®šä¹‰ä¸ºè¯¥ç‚¹åˆ°å…¶æœ€è¿‘èšç±»ä¸­å¿ƒçš„è·ç¦»ã€‚ä¸ºäº†å¾—åˆ°ä¸€ä¸ªéè´Ÿçš„ã€æ˜“äºå¤„ç†çš„è¯¯å·®åº¦é‡ï¼Œæˆ‘ä»¬é€šå¸¸å°†è¿™äº›è·ç¦»å¹³æ–¹åæ±‚å’Œ
  - SSEåœ¨K-meansç®—æ³•ä¸­çš„åº”ç”¨
    1. åˆå§‹åŒ–ï¼š
       - éšæœºé€‰æ‹©Kä¸ªç‚¹ä½œä¸ºåˆå§‹è´¨å¿ƒã€‚
       - è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹åˆ°è¿™äº›è´¨å¿ƒçš„è·ç¦»ï¼Œå¹¶å°†å…¶åˆ†é…ç»™æœ€è¿‘çš„è´¨å¿ƒï¼Œå½¢æˆåˆå§‹èšç±»ã€‚
    2. è¿­ä»£è¿‡ç¨‹ï¼š
       - å¯¹äºæ¯ä¸ªèšç±»ï¼Œé‡æ–°è®¡ç®—è´¨å¿ƒï¼ˆå³è¯¥èšç±»ä¸­æ‰€æœ‰ç‚¹çš„å¹³å‡å€¼ï¼‰ã€‚
       - é‡æ–°åˆ†é…æ¯ä¸ªæ•°æ®ç‚¹åˆ°æœ€è¿‘çš„è´¨å¿ƒã€‚
       - è®¡ç®—æ–°çš„SSEå€¼ã€‚
    3. æ”¶æ•›æ¡ä»¶ï¼š
       - é‡å¤ä¸Šè¿°è¿­ä»£è¿‡ç¨‹ï¼Œç›´åˆ°SSEä¸å†æ˜¾è‘—å‡å°‘ï¼ˆè¾¾åˆ°å±€éƒ¨æˆ–å…¨å±€æœ€å°å€¼ï¼‰ï¼Œæˆ–è€…è¾¾åˆ°é¢„è®¾çš„è¿­ä»£æ¬¡æ•°ã€‚
  - SSEçš„æ€§è´¨
    - SSEæ˜¯ä¸€ä¸ªéè´Ÿå€¼ï¼Œå®ƒè¡¨ç¤ºæ•°æ®ç‚¹åˆ°å…¶æ‰€å±èšç±»ä¸­å¿ƒçš„æ€»ä½“è·ç¦»ã€‚
    - åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼ŒK-meansç®—æ³•éƒ½ä¼šå°è¯•é€šè¿‡é‡æ–°åˆ†é…æ•°æ®ç‚¹å’Œé‡æ–°è®¡ç®—è´¨å¿ƒæ¥å‡å°SSEã€‚
    - ç”±äºåˆå§‹è´¨å¿ƒçš„é€‰æ‹©æ˜¯éšæœºçš„ï¼ŒK-meansç®—æ³•å¯èƒ½ä¼šæ”¶æ•›åˆ°ä¸åŒçš„å±€éƒ¨æœ€å°å€¼ã€‚å› æ­¤ï¼ŒSSEçš„å€¼å¯èƒ½ä¼šå› è¿è¡Œè€Œå¼‚ã€‚
    - SSEçš„å‡å°é€Ÿåº¦é€šå¸¸åœ¨å‰å‡ æ¬¡è¿­ä»£ä¸­è¾ƒå¿«ï¼Œéšç€è¿­ä»£çš„è¿›è¡Œï¼Œè´¨å¿ƒçš„å˜åŒ–é€æ¸å‡å°ï¼ŒSSEçš„å‡å°é€Ÿåº¦ä¹Ÿé€æ¸æ”¾ç¼“ã€‚

##### 2. Two different K-means Clusterings
1. **Optimal Clusteringï¼ˆæœ€ä¼˜èšç±»ï¼‰**
   - æœ€ä¼˜èšç±»æ˜¯æŒ‡K-meansç®—æ³•åœ¨ç»™å®šæ•°æ®é›†ä¸Šèƒ½å¤Ÿæ‰¾åˆ°çš„æœ€ä½³èšç±»æ–¹æ¡ˆã€‚è¿™ä¸ªæ–¹æ¡ˆé€šå¸¸æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š
   1. èšç±»å†…æ•°æ®ç‚¹å°½å¯èƒ½ç›¸ä¼¼ï¼šåœ¨åŒä¸€èšç±»å†…çš„æ•°æ®ç‚¹ä¹‹é—´çš„è·ç¦»åº”è¯¥å°½å¯èƒ½å°ï¼Œè¿™è¡¨ç¤ºæ•°æ®ç‚¹ä¹‹é—´çš„ç›¸ä¼¼æ€§è¾ƒé«˜ã€‚
   2. èšç±»é—´æ•°æ®ç‚¹å°½å¯èƒ½ä¸åŒï¼šä¸åŒèšç±»ä¹‹é—´çš„æ•°æ®ç‚¹è·ç¦»åº”è¯¥å°½å¯èƒ½å¤§ï¼Œè¿™è¡¨ç¤ºèšç±»ä¹‹é—´çš„å·®å¼‚æ€§è¾ƒé«˜ã€‚
   3. SSEæœ€å°ï¼šè¯¯å·®å¹³æ–¹å’Œï¼ˆSSEï¼‰æ˜¯è¡¡é‡èšç±»è´¨é‡çš„ä¸€ä¸ªé‡è¦æŒ‡æ ‡ã€‚åœ¨æœ€ä¼˜èšç±»ä¸­ï¼ŒSSEåº”è¯¥è¾¾åˆ°æœ€å°å€¼ï¼Œè¡¨ç¤ºæ•°æ®ç‚¹åˆ°å…¶æ‰€å±èšç±»ä¸­å¿ƒçš„æ€»ä½“è·ç¦»æœ€å°ã€‚
   - ç„¶è€Œï¼Œç”±äºK-meansç®—æ³•å¯¹åˆå§‹è´¨å¿ƒçš„é€‰æ‹©æ•æ„Ÿï¼Œä¸”å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ï¼Œå› æ­¤åœ¨å®é™…åº”ç”¨ä¸­å¾ˆéš¾ä¿è¯æ€»æ˜¯æ‰¾åˆ°å…¨å±€æœ€ä¼˜çš„èšç±»æ–¹æ¡ˆã€‚

2. **Sub-optimal Clusteringï¼ˆæ¬¡ä¼˜èšç±»ï¼‰**
   -  æ¬¡ä¼˜èšç±»æ˜¯æŒ‡K-meansç®—æ³•åœ¨ç»™å®šæ•°æ®é›†ä¸Šæ‰¾åˆ°çš„èšç±»æ–¹æ¡ˆï¼Œä½†å¹¶éæœ€ä½³æ–¹æ¡ˆã€‚æ¬¡ä¼˜èšç±»å¯èƒ½ç”±äºä»¥ä¸‹åŸå› äº§ç”Ÿï¼š
   1. åˆå§‹è´¨å¿ƒé€‰æ‹©ä¸å½“ï¼šå¦‚æœåˆå§‹è´¨å¿ƒçš„é€‰æ‹©ä¸å¤Ÿåˆç†ï¼ŒK-meansç®—æ³•å¯èƒ½ä¼šæ”¶æ•›åˆ°ä¸€ä¸ªå±€éƒ¨æœ€ä¼˜è§£ï¼Œè€Œä¸æ˜¯å…¨å±€æœ€ä¼˜è§£ã€‚
   2. è¿­ä»£æ¬¡æ•°ä¸è¶³ï¼šå¦‚æœè¿­ä»£æ¬¡æ•°è®¾ç½®å¾—ä¸å¤Ÿå¤šï¼ŒK-meansç®—æ³•å¯èƒ½è¿˜æ²¡æœ‰è¶³å¤Ÿçš„æ—¶é—´æ¥æ‰¾åˆ°æœ€ä¼˜çš„èšç±»æ–¹æ¡ˆå°±å·²ç»åœæ­¢äº†è¿­ä»£ã€‚
   3. æ•°æ®é›†ç‰¹æ€§ï¼šæŸäº›æ•°æ®é›†å¯èƒ½å…·æœ‰å¤æ‚çš„ç»“æ„æˆ–å™ªå£°ï¼Œè¿™ä½¿å¾—K-meansç®—æ³•éš¾ä»¥æ‰¾åˆ°æœ€ä¼˜çš„èšç±»æ–¹æ¡ˆã€‚
   - åœ¨æ¬¡ä¼˜èšç±»ä¸­ï¼ŒSSEçš„å€¼å¯èƒ½ä¼šæ¯”æœ€ä¼˜èšç±»ä¸­çš„SSEå€¼å¤§ï¼Œè¡¨ç¤ºæ•°æ®ç‚¹åˆ°å…¶æ‰€å±èšç±»ä¸­å¿ƒçš„æ€»ä½“è·ç¦»è¾ƒå¤§ï¼Œèšç±»æ•ˆæœè¾ƒå·®ã€‚

3. å¦‚ä½•è¯„ä¼°èšç±»è´¨é‡
   - ä¸ºäº†è¯„ä¼°K-meansèšç±»çš„è´¨é‡ï¼Œé™¤äº†SSEä¹‹å¤–ï¼Œè¿˜å¯ä»¥è€ƒè™‘ä»¥ä¸‹æŒ‡æ ‡ï¼š
   1. è½®å»“ç³»æ•°ï¼ˆSilhouette Coefficientï¼‰ï¼šç”¨äºè¡¡é‡æ•°æ®ç‚¹ä¸å…¶æ‰€å±èšç±»ä¹‹é—´çš„ç´§å¯†ç¨‹åº¦ï¼Œä»¥åŠä¸å…¶ä»–èšç±»ä¹‹é—´çš„åˆ†ç¦»ç¨‹åº¦ã€‚è½®å»“ç³»æ•°çš„å€¼èŒƒå›´åœ¨-1åˆ°1ä¹‹é—´ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºèšç±»æ•ˆæœè¶Šå¥½ã€‚
   2. Calinski-HarabaszæŒ‡æ•°ï¼šä¹Ÿç§°ä¸ºæ–¹å·®æ¯”ç‡å‡†åˆ™ï¼Œç”¨äºè¯„ä¼°èšç±»ç»“æœçš„ç´§å¯†æ€§å’Œåˆ†ç¦»æ€§ã€‚è¯¥æŒ‡æ•°çš„å€¼è¶Šå¤§ï¼Œè¡¨ç¤ºèšç±»æ•ˆæœè¶Šå¥½ã€‚

##### 3. Importance of Choosing Initial Centroids

1. å½±å“èšç±»ç»“æœ
   - åˆå§‹è´¨å¿ƒçš„é€‰æ‹©ä¼šç›´æ¥å½±å“K-meansç®—æ³•çš„èšç±»ç»“æœã€‚ä¸åŒçš„åˆå§‹è´¨å¿ƒå¯èƒ½å¯¼è‡´ç®—æ³•æ”¶æ•›åˆ°ä¸åŒçš„å±€éƒ¨æœ€ä¼˜è§£ï¼Œä»è€Œäº§ç”Ÿä¸åŒçš„èšç±»ç»“æœã€‚å› æ­¤ï¼Œåˆç†çš„åˆå§‹è´¨å¿ƒé€‰æ‹©æœ‰åŠ©äºç®—æ³•æ›´å¿«åœ°æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£æˆ–è¾ƒå¥½çš„å±€éƒ¨æœ€ä¼˜è§£ï¼Œæé«˜èšç±»ç»“æœçš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚
2. å½±å“æ”¶æ•›é€Ÿåº¦
   - åˆå§‹è´¨å¿ƒçš„é€‰æ‹©è¿˜ä¼šå½±å“K-meansç®—æ³•çš„æ”¶æ•›é€Ÿåº¦ã€‚å¦‚æœåˆå§‹è´¨å¿ƒé€‰æ‹©å¾—å½“ï¼Œç®—æ³•å¯ä»¥åœ¨è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°å†…æ”¶æ•›åˆ°ç¨³å®šçš„èšç±»ç»“æœã€‚ç›¸åï¼Œå¦‚æœåˆå§‹è´¨å¿ƒé€‰æ‹©ä¸å½“ï¼Œç®—æ³•å¯èƒ½éœ€è¦æ›´å¤šçš„è¿­ä»£æ¬¡æ•°æ‰èƒ½æ”¶æ•›ï¼Œç”šè‡³å¯èƒ½é™·å…¥æ— æ•ˆçš„è¿­ä»£å¾ªç¯ä¸­ï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚
3. å¯¹å™ªå£°å’Œå¼‚å¸¸å€¼çš„æ•æ„Ÿæ€§
   - K-meansç®—æ³•å¯¹å™ªå£°å’Œå¼‚å¸¸å€¼è¾ƒä¸ºæ•æ„Ÿã€‚å¦‚æœåˆå§‹è´¨å¿ƒé€‰æ‹©ä¸å½“ï¼Œç®—æ³•å¯èƒ½ä¼šå°†å™ªå£°æˆ–å¼‚å¸¸å€¼ä½œä¸ºè´¨å¿ƒï¼Œä»è€Œå¯¼è‡´èšç±»ç»“æœåç¦»å®é™…æ•°æ®åˆ†å¸ƒã€‚å› æ­¤ï¼Œåˆç†çš„åˆå§‹è´¨å¿ƒé€‰æ‹©æœ‰åŠ©äºé™ä½ç®—æ³•å¯¹å™ªå£°å’Œå¼‚å¸¸å€¼çš„æ•æ„Ÿæ€§ï¼Œæé«˜èšç±»ç»“æœçš„é²æ£’æ€§ã€‚

##### 4. Problems with Selecting Initial Points

1. ä½æ¦‚ç‡é€‰æ‹©åˆ°æ¯ä¸ªçœŸå®èšç±»çš„ä¸­å¿ƒï¼š
   1. å½“æ•°æ®é›†å­˜åœ¨Kä¸ªâ€œçœŸå®â€èšç±»æ—¶ï¼Œéšæœºé€‰æ‹©åˆå§‹ç‚¹çš„ç­–ç•¥å¾ˆéš¾ç¡®ä¿æ¯ä¸ªèšç±»éƒ½èƒ½è¢«é€‰ä¸­ä¸€ä¸ªåˆå§‹ç‚¹ã€‚ç‰¹åˆ«æ˜¯å½“Kå€¼è¾ƒå¤§æ—¶ï¼Œè¿™ç§æ¦‚ç‡ä¼šæ˜¾è‘—é™ä½ã€‚
   2. ä¾‹å¦‚ï¼Œå¦‚æœæ¯ä¸ªèšç±»çš„å¤§å°éƒ½æ˜¯nï¼Œä¸”èšç±»ä¹‹é—´ä¸é‡å ï¼Œé‚£ä¹ˆéšæœºé€‰æ‹©Kä¸ªåˆå§‹ç‚¹ï¼Œæ¯ä¸ªç‚¹éƒ½æ¥è‡ªä¸åŒèšç±»çš„æ¦‚ç‡æ˜¯Kä¸ªç‚¹çš„ç»„åˆæ•°é™¤ä»¥æ€»æ•°æ®ç‚¹çš„ç»„åˆæ•°ï¼Œå³K!/(nK)!/[(n(K-1))+K-K!]ï¼ˆè¿™é‡Œç®€åŒ–äº†è®¡ç®—ï¼Œä½†æ ¸å¿ƒæ„æ€æ˜¯æ¦‚ç‡å¾ˆä½ï¼‰ã€‚å½“K=10æ—¶ï¼Œå³ä½¿æ¯ä¸ªèšç±»æœ‰ç›¸åŒçš„æ•°é‡nçš„æ•°æ®ç‚¹ï¼Œè¿™ä¸ªæ¦‚ç‡ä¹Ÿéå¸¸ä½ã€‚
      $$
      P = \frac{\text{number of ways to select one centroid from each cluster}}{\text{number of ways to select } K \text{ centroids}} = \frac{K!n^K}{(Kn)^K} = \frac{K!}{K^K}
      $$
        <p style="display: block;">
         <img src="image_154.png" alt="image_154"/>
        </p>
2. åˆå§‹ç‚¹å¯èƒ½ä¸ç¨³å®šï¼š
   1. åˆå§‹ç‚¹çš„é€‰æ‹©å…·æœ‰éšæœºæ€§ï¼Œè¿™å¯èƒ½å¯¼è‡´ç®—æ³•æ¯æ¬¡è¿è¡Œçš„ç»“æœéƒ½ä¸åŒï¼Œç”šè‡³å¯èƒ½æ”¶æ•›åˆ°ä¸åŒçš„å±€éƒ¨æœ€ä¼˜è§£ã€‚
   2. åˆå§‹ç‚¹çš„ä¸ç¨³å®šé€‰æ‹©ä¼šå½±å“èšç±»çš„ç¨³å®šæ€§å’Œå¯é‡å¤æ€§ã€‚
3. å¯¹å¤§æ•°æ®é›†å’Œå°æ•°æ®é›†çš„æŒ‘æˆ˜ï¼š
   1. å¯¹äºå¤§æ•°æ®é›†ï¼Œéšæœºé€‰æ‹©åˆå§‹ç‚¹å¯èƒ½ä¼šå¯¼è‡´è®¡ç®—å¼€é”€å¤§ï¼Œä¸”éš¾ä»¥ä¿è¯èšç±»è´¨é‡ã€‚
   2. å¯¹äºå°æ•°æ®é›†ï¼Œç”±äºæ•°æ®ç‚¹æ•°é‡æœ‰é™ï¼Œéšæœºé€‰æ‹©åˆå§‹ç‚¹å¯èƒ½æ›´å®¹æ˜“å¯¼è‡´ç®—æ³•é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚

- **åˆå§‹è´¨å¿ƒï¼ˆcentroidï¼‰é€‰æ‹©ç­–ç•¥**
1. æ¯å¯¹èšç±»ä¸­é€‰æ‹©ä¸€ä¸ªèšç±»ï¼Œå¹¶åœ¨å…¶ä¸­æ”¾ç½®ä¸¤ä¸ªåˆå§‹è´¨å¿ƒ
   - è¿™ç§æ–¹æ³•è¯•å›¾é€šè¿‡åœ¨æ¯ä¸ªèšç±»å¯¹ä¸­é€‰æ‹©ä¸€ä¸ªèšç±»å¹¶ä¸ºå…¶åˆ†é…ä¸¤ä¸ªåˆå§‹è´¨å¿ƒæ¥æ”¹è¿›èšç±»è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•æœ‰å‡ ä¸ªæ½œåœ¨çš„é—®é¢˜ï¼š
   1. è´¨å¿ƒé‡å ï¼šå¦‚æœæ‰€é€‰çš„èšç±»éå¸¸ç´§å¯†æˆ–å…·æœ‰ç›¸ä¼¼çš„æ•°æ®åˆ†å¸ƒï¼Œé‚£ä¹ˆä¸¤ä¸ªåˆå§‹è´¨å¿ƒå¯èƒ½ä¼šéå¸¸æ¥è¿‘ï¼Œå¯¼è‡´å®ƒä»¬åœ¨è¿­ä»£è¿‡ç¨‹ä¸­éš¾ä»¥åˆ†ç¦»ï¼Œå¹¶å¯èƒ½æœ€ç»ˆæ”¶æ•›åˆ°åŒä¸€ä¸ªä½ç½®ã€‚
   2. èšç±»ä¸å¹³è¡¡ï¼šå¦‚æœå…¶ä»–èšç±»ä¸­åªåˆ†é…äº†ä¸€ä¸ªåˆå§‹è´¨å¿ƒï¼Œè€Œè¿™äº›èšç±»ä¸­çš„æ•°æ®ç‚¹æ•°é‡æˆ–åˆ†å¸ƒä¸å…·æœ‰ä¸¤ä¸ªè´¨å¿ƒçš„èšç±»ä¸åŒï¼Œé‚£ä¹ˆèšç±»ç»“æœå¯èƒ½ä¼šä¸å¹³è¡¡ã€‚

2. å¤šæ¬¡è¿è¡Œå¹¶é€‰æ‹©æœ€ä¼˜ç»“æœ
   - æ–¹æ³•ï¼šé€šè¿‡å¤šæ¬¡è¿è¡ŒK-meansç®—æ³•ï¼Œæ¯æ¬¡é€‰æ‹©ä¸åŒçš„åˆå§‹è´¨å¿ƒï¼Œç„¶åæ¯”è¾ƒèšç±»ç»“æœçš„è´¨é‡ï¼ˆå¦‚ä½¿ç”¨SSEã€è½®å»“ç³»æ•°ç­‰æŒ‡æ ‡ï¼‰ã€‚é€‰æ‹©è´¨é‡æœ€å¥½çš„ä¸€æ¬¡ä½œä¸ºæœ€ç»ˆèšç±»ç»“æœã€‚
   - ä¼˜ç‚¹ï¼š
     - å¢åŠ äº†æ‰¾åˆ°è¾ƒå¥½èšç±»ç»“æœçš„å¯èƒ½æ€§ã€‚
     - é€‚ç”¨äºå°è§„æ¨¡æ•°æ®é›†æˆ–è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µã€‚
   - ç¼ºç‚¹ï¼š
     - æ¦‚ç‡ä¸Šå¹¶ä¸æ€»æ˜¯èƒ½ä¿è¯æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£ã€‚
     - å¤šæ¬¡è¿è¡Œç®—æ³•å¯èƒ½å¢åŠ è®¡ç®—å¼€é”€ã€‚
3. ä½¿ç”¨ç­–ç•¥é€‰æ‹©åˆå§‹è´¨å¿ƒ
   - æ–¹æ³•ï¼š
     - é€‰æ‹©æœ€å¹¿æ³›åˆ†ç¦»çš„è´¨å¿ƒï¼šé¦–å…ˆéšæœºé€‰æ‹©ä¸€ä¸ªç‚¹ä½œä¸ºç¬¬ä¸€ä¸ªåˆå§‹è´¨å¿ƒï¼Œç„¶åè®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹ä¸å·²é€‰æ‹©çš„è´¨å¿ƒä¹‹é—´çš„æœ€å°è·ç¦»ã€‚æ¥ä¸‹æ¥ï¼Œé€‰æ‹©è·ç¦»å·²é€‰æ‹©è´¨å¿ƒæœ€è¿œçš„ç‚¹ä½œä¸ºä¸‹ä¸€ä¸ªåˆå§‹è´¨å¿ƒï¼Œé‡å¤æ­¤è¿‡ç¨‹ç›´åˆ°é€‰æ‹©äº†kä¸ªåˆå§‹è´¨å¿ƒã€‚è¿™ç§æ–¹æ³•å¯ä»¥ç¡®ä¿åˆå§‹è´¨å¿ƒåœ¨æ•°æ®é›†ä¸­åˆ†å¸ƒè¾ƒä¸ºå‡åŒ€ã€‚
     - ä½¿ç”¨å±‚æ¬¡èšç±»ç¡®å®šåˆå§‹è´¨å¿ƒï¼šé¦–å…ˆä½¿ç”¨å±‚æ¬¡èšç±»ç®—æ³•ï¼ˆå¦‚AGNESæˆ–DIANAï¼‰å¯¹æ•°æ®é›†è¿›è¡Œåˆæ­¥èšç±»ï¼Œç„¶åä»è¿™äº›åˆæ­¥èšç±»ä¸­é€‰æ‹©kä¸ªèšç±»ä¸­å¿ƒä½œä¸ºK-meansç®—æ³•çš„åˆå§‹è´¨å¿ƒã€‚è¿™ç§æ–¹æ³•å¯ä»¥åŸºäºæ•°æ®çš„å±‚æ¬¡ç»“æ„æ¥é€‰æ‹©åˆå§‹è´¨å¿ƒï¼Œä»è€Œå¯èƒ½æé«˜èšç±»ç»“æœçš„å‡†ç¡®æ€§ã€‚
   - ä¼˜ç‚¹ï¼š
     - é€šè¿‡ç­–ç•¥é€‰æ‹©åˆå§‹è´¨å¿ƒï¼Œå¯ä»¥æé«˜èšç±»ç»“æœçš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚
     - å‡å°‘äº†éšæœºé€‰æ‹©åˆå§‹è´¨å¿ƒå¸¦æ¥çš„ä¸ç¡®å®šæ€§ã€‚
   - ç¼ºç‚¹ï¼š
     - å±‚æ¬¡èšç±»ç®—æ³•æœ¬èº«å¯èƒ½å…·æœ‰è¾ƒé«˜çš„è®¡ç®—å¤æ‚åº¦ã€‚
     - éœ€è¦é¢å¤–çš„æ­¥éª¤æ¥ç¡®å®šåˆå§‹è´¨å¿ƒï¼Œå¯èƒ½å¢åŠ ç®—æ³•çš„æ•´ä½“å¤æ‚åº¦ã€‚
4. ä½¿ç”¨K-means++ç®—æ³•
   - æ–¹æ³•ï¼šK-means++ç®—æ³•æ˜¯K-meansç®—æ³•çš„ä¸€ç§æ”¹è¿›ç‰ˆæœ¬ï¼Œå®ƒé€šè¿‡æ™ºèƒ½åœ°é€‰æ‹©åˆå§‹è´¨å¿ƒæ¥æé«˜èšç±»ç»“æœçš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚K-means++ç®—æ³•é¦–å…ˆéšæœºé€‰æ‹©ä¸€ä¸ªç‚¹ä½œä¸ºç¬¬ä¸€ä¸ªåˆå§‹è´¨å¿ƒï¼Œç„¶åå¯¹äºæ¯ä¸ªæœªé€‰æ‹©çš„ç‚¹ï¼Œè®¡ç®—å…¶ä¸å·²é€‰æ‹©çš„åˆå§‹è´¨å¿ƒä¹‹é—´çš„æœ€å°è·ç¦»çš„å¹³æ–¹ï¼Œå¹¶æ ¹æ®è¿™ä¸ªè·ç¦»çš„å¹³æ–¹ä»¥æ­£æ¯”äºæ¦‚ç‡çš„æ–¹å¼é€‰æ‹©ä¸‹ä¸€ä¸ªåˆå§‹è´¨å¿ƒã€‚è¿™ä¸ªè¿‡ç¨‹é‡å¤è¿›è¡Œï¼Œç›´åˆ°é€‰æ‹©äº†kä¸ªåˆå§‹è´¨å¿ƒã€‚
   - ä¼˜ç‚¹ï¼š
     - æ˜¾è‘—æé«˜äº†åˆå§‹è´¨å¿ƒé€‰æ‹©çš„åˆç†æ€§ã€‚
     - å‡å°‘äº†ç®—æ³•é™·å…¥å±€éƒ¨æœ€ä¼˜è§£çš„å¯èƒ½æ€§ã€‚
   - ç¼ºç‚¹ï¼š
     - ç›¸å¯¹äºä¼ ç»Ÿçš„K-meansç®—æ³•ï¼ŒK-means++ç®—æ³•å¯èƒ½å…·æœ‰ç¨é«˜çš„è®¡ç®—å¤æ‚åº¦ã€‚
     - ç»¼ä¸Šæ‰€è¿°ï¼Œé’ˆå¯¹åˆå§‹è´¨å¿ƒé€‰æ‹©çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨å¤šæ¬¡è¿è¡Œå¹¶é€‰æ‹©æœ€ä¼˜ç»“æœã€ä½¿ç”¨ç­–ç•¥é€‰æ‹©åˆå§‹è´¨å¿ƒï¼ˆå¦‚é€‰æ‹©æœ€å¹¿æ³›åˆ†ç¦»çš„è´¨å¿ƒæˆ–ä½¿ç”¨å±‚æ¬¡èšç±»ç¡®å®šåˆå§‹è´¨å¿ƒï¼‰ä»¥åŠä½¿ç”¨K-means++ç®—æ³•ç­‰æ–¹æ³•æ¥æ”¹è¿›K-meansèšç±»ç®—æ³•çš„æ€§èƒ½ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥æ ¹æ®æ•°æ®é›†çš„ç‰¹ç‚¹å’Œè®¡ç®—èµ„æºçš„æƒ…å†µé€‰æ‹©åˆé€‚çš„è§£å†³æ–¹æ¡ˆã€‚

##### 5. Limitations of K-means
- K-means has problems when clusters are of differing
  - Sizes
  - <p style="display: block;">
      <img src="image_155.png" alt="image_155"/>
    </p>
    
  1. å¤„ç†ä¸åŒå¤§å°çš„ç°‡æ—¶çš„å±€é™æ€§
     1. å‡è®¾ç­‰å¤§å°ç°‡ï¼š
        - K-meansç®—æ³•å‡è®¾æ‰€æœ‰ç°‡çš„å¤§å°æ˜¯ç›¸ä¼¼çš„ï¼Œå³æ¯ä¸ªç°‡ä¸­çš„ç‚¹æ•°é‡å¤§è‡´ç›¸åŒã€‚ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œç°‡çš„å¤§å°å¾€å¾€æ˜¯ä¸ç­‰çš„ã€‚
        - å½“ç°‡çš„å¤§å°å·®å¼‚å¾ˆå¤§æ—¶ï¼ŒK-meansç®—æ³•å¯èƒ½ä¼šå°†è¾ƒå°çš„ç°‡æ‹†åˆ†æˆ–åˆå¹¶åˆ°è¾ƒå¤§çš„ç°‡ä¸­ï¼Œå¯¼è‡´èšç±»ç»“æœä¸å‡†ç¡®ã€‚
     2. å¯¹ç°‡å½¢çŠ¶çš„æ•æ„Ÿæ€§ï¼š
        - K-meansç®—æ³•è¿˜å‡è®¾ç°‡æ˜¯ç´§å‡‘å’Œåœ†å½¢çš„ã€‚å¦‚æœç°‡çš„å½¢çŠ¶æ˜¯éåœ†å½¢çš„ï¼ˆå¦‚æ¤­åœ†å½¢ã€é•¿æ¡å½¢ç­‰ï¼‰ï¼ŒK-meansç®—æ³•å¯èƒ½æ— æ³•å‡†ç¡®åœ°æ•æ‰ç°‡çš„ç»“æ„ã€‚
        - è¿™ä¼šå¯¼è‡´èšç±»ç»“æœçš„è¾¹ç•Œä¸æ¸…æ™°ï¼Œä¸”å¯èƒ½æ— æ³•å‡†ç¡®åæ˜ æ•°æ®çš„å®é™…åˆ†å¸ƒã€‚
  2. å¤„ç†åŸå§‹ç‚¹æ—¶çš„å±€é™æ€§
     1. å¯¹åˆå§‹ç‚¹çš„ä¾èµ–æ€§ï¼š
        - K-meansç®—æ³•çš„ç»“æœåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºåˆå§‹ç‚¹çš„é€‰æ‹©ã€‚å¦‚æœåˆå§‹ç‚¹é€‰æ‹©ä¸å½“ï¼Œå¯èƒ½ä¼šå¯¼è‡´èšç±»ç»“æœçš„ä¸ç¨³å®šã€‚
        - ä¸ºäº†å‡è½»è¿™ç§ä¾èµ–æ€§ï¼Œå¯ä»¥ä½¿ç”¨K-means++ç­‰ç®—æ³•æ¥æ”¹è¿›åˆå§‹ç‚¹çš„é€‰æ‹©ã€‚ç„¶è€Œï¼Œå³ä½¿ä½¿ç”¨è¿™äº›æ”¹è¿›ç®—æ³•ï¼Œä»ç„¶æ— æ³•å®Œå…¨æ¶ˆé™¤å¯¹åˆå§‹ç‚¹çš„ä¾èµ–ã€‚
     2. å¯¹å™ªå£°å’Œç¦»ç¾¤ç‚¹çš„æ•æ„Ÿæ€§ï¼š
        - K-meansç®—æ³•å¯¹å™ªå£°å’Œç¦»ç¾¤ç‚¹éå¸¸æ•æ„Ÿã€‚å™ªå£°å’Œç¦»ç¾¤ç‚¹ä¼šå¹²æ‰°ç°‡ä¸­å¿ƒçš„è®¡ç®—ï¼Œä»è€Œå½±å“èšç±»ç»“æœã€‚
        - åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¦‚æœæ•°æ®é›†ä¸­å­˜åœ¨å¤§é‡çš„å™ªå£°æˆ–ç¦»ç¾¤ç‚¹ï¼Œå¯èƒ½éœ€è¦å…ˆå¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œå¦‚ä½¿ç”¨æ»¤æ³¢ã€å»å™ªç­‰æ–¹æ³•æ¥å‡å°‘å™ªå£°å’Œç¦»ç¾¤ç‚¹çš„å½±å“ã€‚
     3. æ— æ³•å¤„ç†éæ•°å€¼å‹æ•°æ®ï¼š
        - K-meansç®—æ³•åªèƒ½å¤„ç†æ•°å€¼å‹æ•°æ®ã€‚å¦‚æœæ•°æ®é›†ä¸­åŒ…å«éæ•°å€¼å‹æ•°æ®ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒç­‰ï¼‰ï¼Œåˆ™éœ€è¦å…ˆè¿›è¡Œé€‚å½“çš„æ•°æ®è½¬æ¢æˆ–é¢„å¤„ç†ï¼Œæ‰èƒ½åº”ç”¨K-meansç®—æ³•ã€‚

  - Densities
  - <p style="display: block;">
     <img src="image_156.png" alt="image_156"/>
    </p>

  - ä¸åŒå¯†åº¦çš„å±€é™æ€§
    1. é—®é¢˜æè¿°ï¼šK-meansç®—æ³•çš„ä¸€ä¸ªä¸»è¦å±€é™æ€§åœ¨äºå®ƒéš¾ä»¥å¤„ç†å¯†åº¦ä¸å‡åŒ€çš„æ•°æ®é›†ã€‚è¯¥ç®—æ³•å‡è®¾æ‰€æœ‰ç°‡çš„å¯†åº¦å¤§è‡´ç›¸åŒï¼Œå³æ¯ä¸ªç°‡ä¸­çš„æ•°æ®ç‚¹æ•°é‡å’Œæ•°æ®ç‚¹ä¹‹é—´çš„ç´§å¯†ç¨‹åº¦ç›¸ä¼¼ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ•°æ®é›†çš„å¯†åº¦å¾€å¾€æ˜¯ä¸å‡åŒ€çš„ï¼ŒæŸäº›ç°‡å¯èƒ½æ¯”å…¶ä»–ç°‡æ›´å¯†é›†æˆ–æ›´ç¨€ç–ã€‚
    2. å½±å“ï¼šå½“æ•°æ®é›†çš„å¯†åº¦ä¸å‡åŒ€æ—¶ï¼ŒK-meansç®—æ³•å¯èƒ½ä¼šå°†ä½å¯†åº¦åŒºåŸŸçš„ç‚¹é”™è¯¯åœ°å½’ç±»åˆ°é«˜å¯†åº¦åŒºåŸŸçš„ç°‡ä¸­ï¼Œæˆ–è€…åœ¨é«˜å¯†åº¦åŒºåŸŸå½¢æˆè¿‡å¤šçš„ç°‡ï¼Œè€Œåœ¨ä½å¯†åº¦åŒºåŸŸå½¢æˆè¾ƒå°‘çš„ç°‡ã€‚è¿™ä¼šå¯¼è‡´èšç±»ç»“æœä¸å‡†ç¡®ï¼Œæ— æ³•åæ˜ æ•°æ®çš„çœŸå®åˆ†å¸ƒã€‚


  - Non-globular shapes
  - <p style="display: block;">
     <img src="image_157.png" alt="image_157"/>
    </p>

- K-means has problems when the data contains outliers.
  - One possible solution is to remove outliers before clustering

##### 6. Over coming K-means Limitations
- é’ˆå¯¹K-meansç®—æ³•åœ¨å¤„ç†å¤æ‚æ•°æ®é›†æ—¶å¯èƒ½å‡ºç°çš„å±€é™æ€§ï¼Œä¸€ç§ç­–ç•¥æ˜¯é¢„å…ˆè®¾å®šä¸€ä¸ªè¾ƒå¤§çš„èšç±»æ•°é‡ï¼Œä½¿å¾—æ¯ä¸ªå°èšç±»éƒ½èƒ½ä»£è¡¨è‡ªç„¶èšç±»çš„ä¸€éƒ¨åˆ†ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•éšåéœ€è¦åœ¨åå¤„ç†é˜¶æ®µå°†è¿™äº›å°èšç±»è¿›ä¸€æ­¥åˆå¹¶ï¼Œä»¥å½¢æˆæ›´ç¬¦åˆå®é™…éœ€æ±‚çš„èšç±»ç»“æœã€‚ä»¥ä¸‹æ˜¯å¯¹è¿™ä¸€ç­–ç•¥çš„è¯¦ç»†é˜è¿°ï¼š
  1. ç­–ç•¥æ¦‚è¿°
     1. åˆæ­¥èšç±»ï¼šé¦–å…ˆï¼Œä½¿ç”¨K-meansç®—æ³•å¯¹æ•°æ®è¿›è¡Œåˆæ­¥èšç±»ï¼Œä½†æ­¤æ—¶è®¾å®šçš„Kå€¼ï¼ˆèšç±»æ•°é‡ï¼‰è¾ƒå¤§ï¼Œç›®çš„æ˜¯ç¡®ä¿æ¯ä¸ªè‡ªç„¶èšç±»éƒ½èƒ½è¢«åˆ†å‰²æˆå¤šä¸ªå°èšç±»ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼Œå³ä½¿è‡ªç„¶èšç±»çš„å½¢çŠ¶ã€å¯†åº¦æˆ–å¤§å°å­˜åœ¨å·®å¼‚ï¼Œä¹Ÿèƒ½åœ¨ä¸€å®šç¨‹åº¦ä¸Šè¢«æ•æ‰åˆ°ã€‚
     2. åå¤„ç†åˆå¹¶ï¼šåœ¨åˆæ­¥èšç±»å®Œæˆåï¼Œéœ€è¦è¿›è¡Œåå¤„ç†æ­¥éª¤ï¼Œå°†è¿™äº›å°èšç±»æ ¹æ®ä¸€å®šçš„è§„åˆ™æˆ–ç›¸ä¼¼æ€§åº¦é‡è¿›è¡Œåˆå¹¶ã€‚è¿™ä¸€æ­¥éª¤å¯èƒ½æ¶‰åŠå¤šç§æŠ€æœ¯å’Œç®—æ³•ï¼Œå¦‚å±‚æ¬¡èšç±»ã€å›¾èšç±»æˆ–åŸºäºå¯†åº¦çš„èšç±»æ–¹æ³•ï¼Œä»¥è¿›ä¸€æ­¥æ•´åˆå’Œä¼˜åŒ–èšç±»ç»“æœã€‚
  2. åå¤„ç†åˆå¹¶çš„æ–¹æ³•
     1. å±‚æ¬¡èšç±»ï¼šå¯ä»¥ä½¿ç”¨å±‚æ¬¡èšç±»æ–¹æ³•ï¼ˆå¦‚å‡èšå±‚æ¬¡èšç±»ï¼‰æ¥åˆå¹¶å°èšç±»ã€‚è¿™ç§æ–¹æ³•é€šè¿‡è®¡ç®—èšç±»é—´çš„ç›¸ä¼¼æ€§ï¼ˆå¦‚è·ç¦»ã€å¯†åº¦ç­‰ï¼‰ï¼Œé€æ­¥å°†æœ€ç›¸ä¼¼çš„å°èšç±»åˆå¹¶æˆå¤§èšç±»ï¼Œç›´åˆ°æ»¡è¶³ä¸€å®šçš„åœæ­¢æ¡ä»¶ã€‚
     2. å›¾èšç±»ï¼šå°†å°èšç±»è§†ä¸ºå›¾ä¸­çš„èŠ‚ç‚¹ï¼Œèšç±»é—´çš„ç›¸ä¼¼æ€§ä½œä¸ºèŠ‚ç‚¹é—´çš„æƒé‡ï¼Œç„¶ååº”ç”¨å›¾èšç±»ç®—æ³•ï¼ˆå¦‚è°±èšç±»ã€æœ€å°å‰²ç®—æ³•ç­‰ï¼‰æ¥åˆå¹¶èŠ‚ç‚¹ï¼ˆå³å°èšç±»ï¼‰ã€‚
     3. åŸºäºå¯†åº¦çš„èšç±»ï¼šå¯ä»¥ä½¿ç”¨åŸºäºå¯†åº¦çš„èšç±»æ–¹æ³•ï¼ˆå¦‚DBSCANï¼‰æ¥è¿›ä¸€æ­¥åˆå¹¶å°èšç±»ã€‚è¿™ç§æ–¹æ³•é€šè¿‡è®¡ç®—æ•°æ®ç‚¹çš„å±€éƒ¨å¯†åº¦ï¼Œå°†å¯†åº¦ç›¸ä¼¼çš„ç‚¹å½’ä¸ºä¸€ç±»ï¼Œä»è€Œå¯èƒ½å°†ç›¸é‚»çš„å°èšç±»åˆå¹¶æˆæ›´å¤§çš„èšç±»ã€‚
     4. æ‰‹åŠ¨åˆå¹¶ï¼šåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¯èƒ½éœ€è¦æ ¹æ®é¢†åŸŸçŸ¥è¯†æˆ–ç‰¹å®šéœ€æ±‚æ‰‹åŠ¨åˆå¹¶å°èšç±»ã€‚è¿™é€šå¸¸æ¶‰åŠå¯¹èšç±»ç»“æœçš„æ·±å…¥åˆ†æå’Œç†è§£ï¼Œä»¥åŠå¯¹æ•°æ®é›†çš„å……åˆ†äº†è§£ã€‚

#### 2. å±‚æ¬¡èšç±»ï¼ˆHierarchical Clusteringï¼‰
- A set of nested clusters organized as a hierarchical tree
- å±‚æ¬¡èšç±»åˆ™æ˜¯åˆ›å»ºä¸€ç»„åµŒå¥—çš„èšç±»ï¼Œè¿™äº›èšç±»è¢«ç»„ç»‡æˆä¸€ä¸ªå±‚æ¬¡æ ‘ï¼ˆæˆ–ç§°ä¸ºèšç±»æ ‘ï¼‰ã€‚åœ¨å±‚æ¬¡æ ‘ä¸­ï¼Œä¸åŒç±»åˆ«çš„åŸå§‹æ•°æ®ç‚¹æ˜¯æ ‘çš„æœ€ä½å±‚ï¼Œè€Œæ ‘çš„é¡¶å±‚åˆ™æ˜¯ä¸€ä¸ªèšç±»çš„æ ¹èŠ‚ç‚¹ã€‚å±‚æ¬¡èšç±»å¯ä»¥é€šè¿‡è‡ªä¸‹è€Œä¸Šçš„åˆå¹¶ï¼ˆå‡èšæ³•ï¼‰æˆ–è‡ªä¸Šè€Œä¸‹çš„åˆ†è£‚ï¼ˆåˆ†è£‚æ³•ï¼‰æ¥æ„å»ºèšç±»æ ‘ã€‚
- å±‚æ¬¡èšç±»å‡è®¾æ•°æ®ç±»åˆ«ä¹‹é—´å­˜åœ¨å±‚æ¬¡ç»“æ„ï¼Œé€šè¿‡å¯¹æ•°æ®é›†åœ¨ä¸åŒå±‚æ¬¡çš„åˆ’åˆ†ï¼Œæ„é€ å‡ºæ ‘çŠ¶ç»“æ„çš„èšç±»ç»“æœã€‚è¿™ç§æ–¹æ³•ä¸éœ€è¦é¢„å…ˆæŒ‡å®šèšç±»æ•°é‡ï¼Œè€Œæ˜¯é€æ­¥åˆå¹¶æˆ–åˆ†è£‚èšç±»ï¼Œä»¥æ­ç¤ºæ•°æ®çš„å†…åœ¨ç»“æ„å’Œå±‚æ¬¡å…³ç³»ã€‚

<p style="display: block;">
  <img src="image_153.png" alt="image_153"/>
</p>

##### 1. Strengths of Hierarchical Clustering
1. æ— éœ€é¢„è®¾ç°‡çš„æ•°é‡
   - å±‚æ¬¡èšç±»çš„ä¸€ä¸ªæ˜¾è‘—ä¼˜åŠ¿æ˜¯ï¼Œå®ƒä¸éœ€è¦åœ¨å¼€å§‹æ—¶å°±é¢„è®¾æ•°æ®é›†ä¸­ç°‡ï¼ˆæˆ–ç¾¤ç»„ï¼‰çš„æ•°é‡ã€‚è¿™ä¸K-meansç­‰éœ€è¦äº‹å…ˆæŒ‡å®šç°‡æ•°é‡çš„èšç±»ç®—æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚
   - åœ¨å±‚æ¬¡èšç±»ä¸­ï¼Œç°‡çš„æ•°é‡æ˜¯åœ¨èšç±»è¿‡ç¨‹ç»“æŸåï¼Œæ ¹æ®å®é™…éœ€æ±‚é€šè¿‡â€œåˆ‡å‰²â€æ ‘çŠ¶å›¾ï¼ˆdendrogramï¼‰æ¥ç¡®å®šçš„ã€‚è¿™æ„å‘³ç€ç”¨æˆ·å¯ä»¥æ ¹æ®æ•°æ®çš„ç‰¹æ€§å’Œåˆ†æçš„ç›®æ ‡ï¼Œçµæ´»åœ°é€‰æ‹©æ‰€éœ€çš„ç°‡æ•°é‡ã€‚
2. å¯è·å¾—ä»»æ„æ•°é‡çš„ç°‡
   - é€šè¿‡åœ¨æ ‘çŠ¶å›¾çš„é€‚å½“ä½ç½®è¿›è¡Œâ€œåˆ‡å‰²â€ï¼Œç”¨æˆ·å¯ä»¥è½»æ¾åœ°è·å¾—æ‰€éœ€æ•°é‡çš„ç°‡ã€‚è¿™ç§çµæ´»æ€§ä½¿å¾—å±‚æ¬¡èšç±»åœ¨æ¢ç´¢æ€§æ•°æ®åˆ†æä¸­ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºç”¨æˆ·å¯ä»¥åœ¨ä¸åŒçš„ç°‡æ•°é‡ä¹‹é—´è¿›è¡Œæ¯”è¾ƒï¼Œä»¥æ‰¾åˆ°æœ€ä½³æˆ–æœ€æœ‰æ„ä¹‰çš„èšç±»ç»“æœã€‚
3. å¯èƒ½å¯¹åº”æœ‰æ„ä¹‰çš„åˆ†ç±»ä½“ç³»
   - å±‚æ¬¡èšç±»çš„ç»“æœå¯èƒ½å¯¹åº”äºæ•°æ®é›†ä¸­å­˜åœ¨çš„æœ‰æ„ä¹‰åˆ†ç±»ä½“ç³»ã€‚ä¾‹å¦‚ï¼Œåœ¨ç”Ÿç‰©ç§‘å­¦é¢†åŸŸï¼Œå±‚æ¬¡èšç±»å¯ä»¥ç”¨äºé‡å»ºåŠ¨ç‰©ç•Œçš„åˆ†ç±»ç³»ç»Ÿï¼ˆå¦‚åŠ¨ç‰©ç‹å›½ä¸­çš„ç‰©ç§åˆ†ç±»ï¼‰æˆ–æ„å»ºç”Ÿç‰©è¿›åŒ–æ ‘ï¼ˆphylogenyï¼‰ã€‚
   - åœ¨è¿™äº›åº”ç”¨ä¸­ï¼Œæ ‘çŠ¶å›¾ä¸ä»…å±•ç¤ºäº†æ•°æ®ç‚¹ä¹‹é—´çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§ï¼Œè¿˜æ­ç¤ºäº†å®ƒä»¬ä¹‹é—´çš„æ½œåœ¨å…³ç³»æˆ–è¿›åŒ–å†å²ã€‚
4. ç”Ÿç‰©ç§‘å­¦ä¸­çš„åº”ç”¨å®ä¾‹
   - åœ¨ç”Ÿç‰©ç§‘å­¦ä¸­ï¼Œå±‚æ¬¡èšç±»è¢«å¹¿æ³›ç”¨äºåˆ†æåŸºå› è¡¨è¾¾æ•°æ®ã€è›‹ç™½è´¨ç›¸äº’ä½œç”¨ç½‘ç»œã€ç‰©ç§åˆ†ç±»å’Œè¿›åŒ–ç ”ç©¶ç­‰é¢†åŸŸã€‚
   - ä¾‹å¦‚ï¼Œé€šè¿‡å±‚æ¬¡èšç±»ï¼Œç ”ç©¶äººå‘˜å¯ä»¥è¯†åˆ«å‡ºå…·æœ‰ç›¸ä¼¼è¡¨è¾¾æ¨¡å¼çš„åŸºå› ç¾¤ï¼Œè¿™äº›åŸºå› ç¾¤å¯èƒ½ä¸ç‰¹å®šçš„ç”Ÿç‰©è¿‡ç¨‹æˆ–ç–¾ç—…çŠ¶æ€ç›¸å…³ã€‚
   - åŒæ ·åœ°ï¼Œåœ¨ç‰©ç§åˆ†ç±»å’Œè¿›åŒ–ç ”ç©¶ä¸­ï¼Œå±‚æ¬¡èšç±»å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ„å»ºæ›´å‡†ç¡®çš„åˆ†ç±»ç³»ç»Ÿå’Œè¿›åŒ–æ ‘ï¼Œä»è€Œæ›´æ·±å…¥åœ°äº†è§£ç”Ÿç‰©å¤šæ ·æ€§å’Œè¿›åŒ–å†å²ã€‚
- ç»¼ä¸Šæ‰€è¿°ï¼Œå±‚æ¬¡èšç±»å…·æœ‰æ— éœ€é¢„è®¾ç°‡æ•°é‡ã€å¯è·å¾—ä»»æ„æ•°é‡çš„ç°‡ä»¥åŠå¯èƒ½å¯¹åº”æœ‰æ„ä¹‰çš„åˆ†ç±»ä½“ç³»ç­‰æ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™äº›ä¼˜åŠ¿ä½¿å¾—å±‚æ¬¡èšç±»åœ¨æ•°æ®åˆ†æé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œé‡è¦çš„ç ”ç©¶ä»·å€¼ã€‚

##### 2. Two Types of Hierarchical Clustering
1. å‡èšå‹å±‚æ¬¡èšç±»ï¼ˆAgglomerative Hierarchical Clusteringï¼‰
   - èµ·å§‹ç‚¹ï¼šæ¯ä¸ªæ•°æ®ç‚¹è¢«è§†ä¸ºä¸€ä¸ªå•ç‹¬çš„ç°‡ã€‚
   - é€æ­¥åˆå¹¶ï¼šåœ¨æ¯ä¸€æ­¥ä¸­ï¼Œéƒ½ä¼šæ‰¾åˆ°å¹¶åˆå¹¶æœ€æ¥è¿‘ï¼ˆæˆ–æœ€ç›¸ä¼¼ï¼‰çš„ä¸€å¯¹ç°‡ã€‚è¿™é€šå¸¸åŸºäºæŸç§è·ç¦»åº¦é‡ï¼ˆå¦‚æ¬§å‡ é‡Œå¾—è·ç¦»ï¼‰æˆ–ç›¸ä¼¼æ€§åº¦é‡ï¼ˆå¦‚ä½™å¼¦ç›¸ä¼¼æ€§ï¼‰æ¥ç¡®å®šã€‚
   - ç»ˆæ­¢æ¡ä»¶ï¼šåˆå¹¶è¿‡ç¨‹ä¼šä¸€ç›´è¿›è¡Œï¼Œç›´åˆ°æ‰€æœ‰æ•°æ®ç‚¹éƒ½è¢«åˆå¹¶æˆä¸€ä¸ªå•ä¸€çš„ç°‡ï¼Œæˆ–è€…æ ¹æ®éœ€æ±‚åœ¨è¾¾åˆ°æŸä¸ªæŒ‡å®šçš„ç°‡æ•°é‡æ—¶åœæ­¢ã€‚
   - ç»“æœè¡¨ç¤ºï¼šç»“æœé€šå¸¸è¡¨ç¤ºä¸ºä¸€ä¸ªæ ‘çŠ¶å›¾ï¼ˆdendrogramï¼‰ï¼Œå®ƒè®°å½•äº†ç°‡çš„åˆå¹¶åºåˆ—å’Œæ¯ä¸ªåˆå¹¶æ­¥éª¤çš„è·ç¦»æˆ–ç›¸ä¼¼æ€§åº¦é‡ã€‚

   - Key Idea: Successively merge closest clusters
   - Basic algorithm
     1. Compute the proximity matrix
     2. Let each data point be a cluster
     3. Repeat
     4. Merge the two closest clusters
     5. Update the proximity matrix
     6. Until only a single cluster remains
   - Key operation is the computation of the proximity of two clusters
     - Different approaches to defining the distance between clusters distinguish the different algorithms
     
   - How to define Inter-cluster distance
     1. MINï¼ˆå•é“¾æ³•ï¼‰
        - å®šä¹‰ï¼šä¸¤ä¸ªç°‡ä¹‹é—´çš„æœ€çŸ­è·ç¦»è¢«å®šä¹‰ä¸ºè¿™ä¸¤ä¸ªç°‡ä¸­ä»»æ„ä¸¤ç‚¹ä¹‹é—´çš„æœ€çŸ­è·ç¦»ã€‚
        - ç‰¹ç‚¹ï¼šå•é“¾æ³•å€¾å‘äºå½¢æˆé•¿é“¾å½¢çš„ç°‡ï¼Œå› ä¸ºå®ƒåªå…³æ³¨æœ€è¿‘çš„ç‚¹å¯¹ã€‚
        - é€‚ç”¨åœºæ™¯ï¼šé€‚ç”¨äºæ•°æ®ç‚¹åˆ†å¸ƒä¸å‡åŒ€æˆ–å­˜åœ¨å™ªå£°çš„æƒ…å†µï¼Œä½†å¯èƒ½å¯¹å™ªå£°å’Œç¦»ç¾¤ç‚¹æ•æ„Ÿã€‚
        - <p style="display: block;">
            <img src="image_158.png" alt="image_158"/>
          </p>
          
          1. åµŒå¥—ç°‡ï¼ˆNested Clustersï¼‰
             - å±‚æ¬¡èšç±»ç”Ÿæˆçš„ç°‡ç»“æ„æ˜¯åµŒå¥—çš„ï¼Œå³ä¸€ä¸ªç°‡å¯ä»¥åŒ…å«å…¶ä»–å­ç°‡ã€‚è¿™ç§åµŒå¥—ç»“æ„ä½¿å¾—å±‚æ¬¡èšç±»èƒ½å¤Ÿæ­ç¤ºæ•°æ®çš„æ·±å±‚ç»“æ„å’Œå…³ç³»ã€‚åœ¨å‡èšå‹å±‚æ¬¡èšç±»ä¸­ï¼Œæ¯ä¸ªæ•°æ®ç‚¹æœ€åˆéƒ½è¢«è§†ä¸ºä¸€ä¸ªå•ç‹¬çš„ç°‡ï¼Œç„¶åé€šè¿‡ä¸æ–­åœ°åˆå¹¶æœ€è¿‘çš„ä¸¤ä¸ªç°‡ï¼Œæœ€ç»ˆå½¢æˆä¸€ä¸ªåŒ…å«æ‰€æœ‰æ•°æ®ç‚¹çš„å•ä¸€å¤§ç°‡ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥å½¢æˆå¤šä¸ªä¸­é—´å±‚æ¬¡çš„ç°‡ï¼Œè¿™äº›ç°‡æ„æˆäº†åµŒå¥—ç°‡ç»“æ„ã€‚
          2. æ ‘çŠ¶å›¾ï¼ˆDendrogramï¼‰
             - æ ‘çŠ¶å›¾æ˜¯å±‚æ¬¡èšç±»ç»“æœçš„å¯è§†åŒ–è¡¨ç¤ºï¼Œå®ƒå±•ç¤ºäº†æ•°æ®ç‚¹æˆ–ç°‡ä¹‹é—´çš„å±‚æ¬¡å…³ç³»ã€‚åœ¨æ ‘çŠ¶å›¾ä¸­ï¼Œæ¯ä¸ªæ•°æ®ç‚¹æˆ–ç°‡éƒ½è¡¨ç¤ºä¸ºä¸€ä¸ªèŠ‚ç‚¹ï¼ŒèŠ‚ç‚¹ä¹‹é—´çš„è¿çº¿è¡¨ç¤ºå®ƒä»¬ä¹‹é—´çš„å±‚æ¬¡å…³ç³»ã€‚æ ‘çŠ¶å›¾é€šå¸¸å…·æœ‰ä»¥ä¸‹å‡ ä¸ªç‰¹ç‚¹ï¼š
             - å±‚æ¬¡ç»“æ„ï¼šæ ‘çŠ¶å›¾æ¸…æ™°åœ°å±•ç¤ºäº†æ•°æ®ç‚¹æˆ–ç°‡ä¹‹é—´çš„å±‚æ¬¡å…³ç³»ï¼Œä½¿å¾—ç ”ç©¶è€…å¯ä»¥ç›´è§‚åœ°çœ‹åˆ°å“ªäº›æ•°æ®ç‚¹æˆ–ç°‡è¢«åˆå¹¶åœ¨ä¸€èµ·ï¼Œä»¥åŠå®ƒä»¬åˆå¹¶çš„é¡ºåºã€‚
             - è·ç¦»åº¦é‡ï¼šæ ‘çŠ¶å›¾ä¸­çš„è¿çº¿é•¿åº¦é€šå¸¸è¡¨ç¤ºèŠ‚ç‚¹ä¹‹é—´çš„è·ç¦»æˆ–ç›¸ä¼¼åº¦ã€‚åœ¨å‡èšå‹å±‚æ¬¡èšç±»ä¸­ï¼Œè¿çº¿é•¿åº¦é€šå¸¸è¡¨ç¤ºåˆå¹¶æ—¶ä¸¤ä¸ªç°‡ä¹‹é—´çš„è·ç¦»æˆ–ç›¸ä¼¼åº¦åº¦é‡ã€‚
             - å¯è¯»æ€§ï¼šé€šè¿‡é€‚å½“çš„ç¼©æ”¾å’Œå¸ƒå±€ï¼Œæ ‘çŠ¶å›¾å¯ä»¥å˜å¾—æ˜“äºé˜…è¯»å’Œç†è§£ã€‚ç ”ç©¶è€…å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´æ ‘çŠ¶å›¾çš„æ˜¾ç¤ºå‚æ•°ï¼Œä»¥ä¾¿æ›´å¥½åœ°å±•ç¤ºæ•°æ®çš„å±‚æ¬¡ç»“æ„ã€‚
             - åœ¨å±‚æ¬¡èšç±»åˆ†æä¸­ï¼Œæ ‘çŠ¶å›¾æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„å·¥å…·ï¼Œå®ƒä½¿å¾—ç ”ç©¶è€…èƒ½å¤Ÿç›´è§‚åœ°çœ‹åˆ°æ•°æ®çš„èšç±»è¿‡ç¨‹å’Œç»“æœï¼Œä»è€Œæ›´æ·±å…¥åœ°ç†è§£æ•°æ®çš„ç»“æ„å’Œå…³ç³»ã€‚
        - **Strength of MIN**
        - å¤„ç†éæ¤­åœ†å½¢å½¢çŠ¶ï¼š
          1. MINæ–¹æ³•é€šè¿‡è®¡ç®—ä¸¤ä¸ªç°‡ä¸­è·ç¦»æœ€è¿‘çš„ä¸¤ä¸ªç‚¹ä¹‹é—´çš„è·ç¦»æ¥ç¡®å®šç°‡é—´çš„ç›¸ä¼¼åº¦ã€‚è¿™ç§è®¡ç®—æ–¹å¼ä½¿å¾—MINæ–¹æ³•èƒ½å¤Ÿæ›´çµæ´»åœ°å¤„ç†å½¢çŠ¶ä¸è§„åˆ™çš„æ•°æ®é›†ï¼Œç‰¹åˆ«æ˜¯é‚£äº›ä¸ç¬¦åˆæ¤­åœ†å½¢åˆ†å¸ƒçš„æ•°æ®ã€‚
          2. ç›¸æ¯”ä¹‹ä¸‹ï¼Œå…¶ä»–èšç±»æ–¹æ³•ï¼ˆå¦‚K-meansï¼‰å¯èƒ½æ›´é€‚åˆå¤„ç†æ¤­åœ†å½¢æˆ–çƒå½¢åˆ†å¸ƒçš„æ•°æ®é›†ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸åŸºäºæ•°æ®ç‚¹åˆ°ç°‡ä¸­å¿ƒçš„è·ç¦»æ¥è¿›è¡Œèšç±»ã€‚
        - å¯¹å™ªå£°å’Œå¼‚å¸¸å€¼çš„é²æ£’æ€§ï¼š
          1. å°½ç®¡MINæ–¹æ³•å¯¹å™ªå£°å’Œå¼‚å¸¸å€¼è¾ƒä¸ºæ•æ„Ÿï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿™ç§æ•æ„Ÿæ€§å¯ä»¥è¢«è§†ä¸ºä¸€ç§ä¼˜åŠ¿ã€‚ä¾‹å¦‚ï¼Œåœ¨æ•°æ®é›†ä¸­å­˜åœ¨å°‘é‡å™ªå£°æˆ–å¼‚å¸¸å€¼æ—¶ï¼ŒMINæ–¹æ³•ä»ç„¶èƒ½å¤Ÿæ‰¾åˆ°æ•°æ®ä¸­çš„ä¸»è¦ç»“æ„ï¼Œå› ä¸ºè¿™äº›å™ªå£°æˆ–å¼‚å¸¸å€¼é€šå¸¸ä¸ä¼šå¯¹è·ç¦»æœ€è¿‘çš„ç‚¹å¯¹äº§ç”Ÿå¤ªå¤§å½±å“ã€‚
          2. å½“ç„¶ï¼Œå¦‚æœå™ªå£°æˆ–å¼‚å¸¸å€¼è¿‡å¤šï¼ŒMINæ–¹æ³•å¯èƒ½ä¼šå—åˆ°è¾ƒå¤§å½±å“ï¼Œå¯¼è‡´èšç±»ç»“æœä¸å‡†ç¡®ã€‚ä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥é€šè¿‡æ•°æ®é¢„å¤„ç†å’Œå‚æ•°è°ƒæ•´æ¥å‡è½»è¿™ç§å½±å“ã€‚
        - ç›´è§‚æ€§å’Œæ˜“äºç†è§£ï¼š
          1. MINæ–¹æ³•çš„èšç±»è¿‡ç¨‹ç›¸å¯¹ç›´è§‚ï¼Œå› ä¸ºå®ƒåŸºäºè·ç¦»æœ€è¿‘çš„ç‚¹å¯¹è¿›è¡Œèšç±»ã€‚è¿™ä½¿å¾—ç ”ç©¶è€…èƒ½å¤Ÿæ›´å®¹æ˜“åœ°ç†è§£èšç±»ç»“æœå’Œæ•°æ®çš„ç»“æ„ã€‚
          2. æ­¤å¤–ï¼ŒMINæ–¹æ³•è¿˜å¯ä»¥ä¸å…¶ä»–æ–¹æ³•ï¼ˆå¦‚å¯è§†åŒ–æŠ€æœ¯ï¼‰ç›¸ç»“åˆï¼Œä»¥æä¾›æ›´ä¸°å¯Œçš„èšç±»ä¿¡æ¯å’Œåˆ†æç»“æœã€‚
        
        - **Weakness of MIN**
        - å¯¹å™ªå£°çš„æ•æ„Ÿæ€§
        - æ˜“å—å™ªå£°ç‚¹å½±å“ï¼š
          1. MINæ–¹æ³•é€šè¿‡è®¡ç®—ä¸¤ä¸ªç°‡ä¸­è·ç¦»æœ€è¿‘çš„ä¸¤ä¸ªç‚¹ä¹‹é—´çš„è·ç¦»æ¥ç¡®å®šç°‡é—´çš„ç›¸ä¼¼åº¦ã€‚è¿™ç§è®¡ç®—æ–¹å¼ä½¿å¾—MINæ–¹æ³•å¯¹å™ªå£°ç‚¹éå¸¸æ•æ„Ÿã€‚å½“æ•°æ®é›†ä¸­å­˜åœ¨å™ªå£°ç‚¹æ—¶ï¼Œè¿™äº›ç‚¹å¯èƒ½ä¼šæˆä¸ºè·ç¦»æœ€è¿‘çš„ç‚¹å¯¹ï¼Œä»è€Œå¯¼è‡´é”™è¯¯çš„èšç±»ç»“æœã€‚
          2. å™ªå£°ç‚¹å¯èƒ½ä¼šè¯¯å¯¼MINæ–¹æ³•å°†åŸæœ¬ä¸ç›¸ä¼¼çš„ç°‡åˆå¹¶åœ¨ä¸€èµ·ï¼Œæˆ–è€…å°†ç›¸ä¼¼çš„ç°‡åˆ†å¼€ã€‚è¿™é™ä½äº†èšç±»ç»“æœçš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚
        - éš¾ä»¥å¤„ç†ç¦»ç¾¤ç‚¹ï¼š
          1. ç¦»ç¾¤ç‚¹æ˜¯æŒ‡ä¸æ•°æ®é›†ä¸­å¤§å¤šæ•°ç‚¹è·ç¦»è¾ƒè¿œçš„ç‚¹ã€‚åœ¨MINæ–¹æ³•ä¸­ï¼Œç¦»ç¾¤ç‚¹å¯èƒ½ä¼šæˆä¸ºå½±å“èšç±»ç»“æœçš„å…³é”®å› ç´ ã€‚
          2. ç”±äºMINæ–¹æ³•åªå…³æ³¨è·ç¦»æœ€è¿‘çš„ç‚¹å¯¹ï¼Œå› æ­¤ç¦»ç¾¤ç‚¹å¯èƒ½ä¼šä¸è·ç¦»è¾ƒè¿‘çš„ç°‡å½¢æˆé”™è¯¯çš„é“¾æ¥ï¼Œä»è€Œç ´åæ•°æ®çš„æ•´ä½“ç»“æ„ã€‚

     2. MAXï¼ˆå…¨é“¾æ³•ï¼‰
        - å®šä¹‰ï¼šä¸¤ä¸ªç°‡ä¹‹é—´çš„æœ€é•¿è·ç¦»è¢«å®šä¹‰ä¸ºè¿™ä¸¤ä¸ªç°‡ä¸­ä»»æ„ä¸¤ç‚¹ä¹‹é—´çš„æœ€é•¿è·ç¦»ã€‚
        - ç‰¹ç‚¹ï¼šå…¨é“¾æ³•å€¾å‘äºå½¢æˆç´§å‡‘çš„çƒå½¢ç°‡ï¼Œå› ä¸ºå®ƒå…³æ³¨æœ€è¿œçš„ç‚¹å¯¹ã€‚
        - é€‚ç”¨åœºæ™¯ï¼šé€‚ç”¨äºæ•°æ®ç‚¹åˆ†å¸ƒè¾ƒä¸ºå‡åŒ€ä¸”å¸Œæœ›å½¢æˆç´§å‡‘ç°‡çš„æƒ…å†µã€‚
        - <p style="display: block;">
            <img src="image_159.png" alt="image_159"/>
          </p>

        - **Strength of MAX**
        - Less susceptible to noise

        - **Weakness of MAX**
        - Tends to break large clusters
        - Biased towards globular clusters
         
     3. Group Averageï¼ˆå¹³å‡é“¾æ³•ï¼‰
        - å®šä¹‰ï¼šä¸¤ä¸ªç°‡ä¹‹é—´çš„å¹³å‡è·ç¦»è¢«å®šä¹‰ä¸ºè¿™ä¸¤ä¸ªç°‡ä¸­æ‰€æœ‰ç‚¹å¯¹è·ç¦»çš„å¹³å‡å€¼ã€‚
        - ç‰¹ç‚¹ï¼šå¹³å‡é“¾æ³•è€ƒè™‘äº†ç°‡å†…æ‰€æœ‰ç‚¹å¯¹çš„è·ç¦»ï¼Œå› æ­¤èƒ½å¤Ÿæ›´å…¨é¢åœ°åæ˜ ç°‡é—´çš„ç›¸ä¼¼æ€§ã€‚
        - é€‚ç”¨åœºæ™¯ï¼šé€‚ç”¨äºæ•°æ®ç‚¹åˆ†å¸ƒè¾ƒä¸ºå‡åŒ€ä¸”å¸Œæœ›å½¢æˆå¹³è¡¡ç°‡çš„æƒ…å†µã€‚
        - <p style="display: block;">
            <img src="image_160.png" alt="image_160"/>
          </p>

        - **Strength of Average**
        - Less susceptible to noise

        - **Weakness of Average**
        - Biased towards globular clusters
         
     4. Distance Between Centroidsï¼ˆè´¨å¿ƒè·ç¦»æ³•ï¼‰
        - å®šä¹‰ï¼šä¸¤ä¸ªç°‡ä¹‹é—´çš„è´¨å¿ƒè·ç¦»è¢«å®šä¹‰ä¸ºè¿™ä¸¤ä¸ªç°‡è´¨å¿ƒä¹‹é—´çš„è·ç¦»ã€‚
        - ç‰¹ç‚¹ï¼šè´¨å¿ƒè·ç¦»æ³•è®¡ç®—ç®€å•ï¼Œä½†å¯èƒ½å—åˆ°ç°‡å½¢çŠ¶å’Œå¤§å°çš„å½±å“ã€‚
        - é€‚ç”¨åœºæ™¯ï¼šé€‚ç”¨äºç°‡å½¢çŠ¶å’Œå¤§å°è¾ƒä¸ºä¸€è‡´çš„æƒ…å†µã€‚
     5. Wardâ€™s Methodï¼ˆæ²ƒå¾·æ–¹æ³•ï¼‰
        - å®šä¹‰ï¼šWardæ–¹æ³•é€šè¿‡è®¡ç®—åˆå¹¶ä¸¤ä¸ªç°‡åæ€»ä½“å†…èšç±»æ–¹å·®å¢åŠ çš„é‡æ¥ç¡®å®šç°‡é—´è·ç¦»ã€‚
        - ç‰¹ç‚¹ï¼šWardæ–¹æ³•å€¾å‘äºå½¢æˆç´§å‡‘ä¸”æ–¹å·®è¾ƒå°çš„ç°‡ï¼Œå› ä¸ºå®ƒå…³æ³¨åˆå¹¶åçš„SSEå¢é‡ã€‚
        - é€‚ç”¨åœºæ™¯ï¼šé€‚ç”¨äºå¸Œæœ›å½¢æˆç´§å‡‘ä¸”æ–¹å·®è¾ƒå°çš„ç°‡çš„æƒ…å†µã€‚
        - **Strength of Ward**
        - Less susceptible to noise

        - **Weakness of Ward**
        - Biased towards globular clusters

2. åˆ†è£‚å‹å±‚æ¬¡èšç±»ï¼ˆDivisive Hierarchical Clusteringï¼‰
   - èµ·å§‹ç‚¹ï¼šæ‰€æœ‰æ•°æ®ç‚¹éƒ½è¢«è§†ä¸ºä¸€ä¸ªæ•´ä½“ç°‡ã€‚
   - é€æ­¥åˆ†è£‚ï¼šåœ¨æ¯ä¸€æ­¥ä¸­ï¼Œéƒ½ä¼šé€‰æ‹©ä¸€ä¸ªç°‡å¹¶å°†å…¶åˆ†è£‚æˆä¸¤ä¸ªè¾ƒå°çš„ç°‡ã€‚è¿™é€šå¸¸åŸºäºæŸç§å·®å¼‚æ€§åº¦é‡ï¼ˆå¦‚ç°‡å†…æ•°æ®ç‚¹çš„è·ç¦»ï¼‰æ¥ç¡®å®šã€‚
   - ç»ˆæ­¢æ¡ä»¶ï¼šåˆ†è£‚è¿‡ç¨‹ä¼šä¸€ç›´è¿›è¡Œï¼Œç›´åˆ°æ¯ä¸ªç°‡éƒ½åªåŒ…å«ä¸€ä¸ªæ•°æ®ç‚¹ï¼Œæˆ–è€…æ ¹æ®éœ€æ±‚åœ¨è¾¾åˆ°æŸä¸ªæŒ‡å®šçš„ç°‡æ•°é‡æ—¶åœæ­¢ã€‚
   - ç»“æœè¡¨ç¤ºï¼šä¸å‡èšå‹å±‚æ¬¡èšç±»ç±»ä¼¼ï¼Œç»“æœä¹Ÿå¯ä»¥è¡¨ç¤ºä¸ºä¸€ä¸ªæ ‘çŠ¶å›¾ï¼Œä½†å®ƒæ˜¯ä¸€ä¸ªå€’ç½®çš„æ ‘ï¼Œè®°å½•äº†ç°‡çš„åˆ†è£‚åºåˆ—å’Œæ¯ä¸ªåˆ†è£‚æ­¥éª¤çš„å·®å¼‚æ€§åº¦é‡ã€‚
3. ä¼ ç»Ÿå±‚æ¬¡ç®—æ³•ä¸­çš„ç›¸ä¼¼æ€§æˆ–è·ç¦»çŸ©é˜µ
   - åœ¨è¿›è¡Œå±‚æ¬¡èšç±»ä¹‹å‰ï¼Œé€šå¸¸éœ€è¦è®¡ç®—æ¯å¯¹æ•°æ®ç‚¹ä¹‹é—´çš„ç›¸ä¼¼æ€§æˆ–è·ç¦»çŸ©é˜µã€‚è¿™ä¸ªçŸ©é˜µç”¨äºè¡¡é‡æ•°æ®ç‚¹ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä½œä¸ºåç»­åˆå¹¶æˆ–åˆ†è£‚æ­¥éª¤çš„ä¾æ®ã€‚
   - å¸¸ç”¨çš„è·ç¦»åº¦é‡åŒ…æ‹¬æ¬§å‡ é‡Œå¾—è·ç¦»ã€æ›¼å“ˆé¡¿è·ç¦»ç­‰ï¼Œè€Œç›¸ä¼¼æ€§åº¦é‡åˆ™å¯èƒ½åŒ…æ‹¬ä½™å¼¦ç›¸ä¼¼æ€§ã€çš®å°”é€Šç›¸å…³ç³»æ•°ç­‰ã€‚
4. åˆå¹¶æˆ–åˆ†è£‚çš„è´ªå©ªç­–ç•¥
   - æ— è®ºæ˜¯å‡èšå‹è¿˜æ˜¯åˆ†è£‚å‹å±‚æ¬¡èšç±»ï¼Œéƒ½ä½¿ç”¨äº†ä¸€ç§è´ªå©ªçš„ç­–ç•¥æ¥é€æ­¥åˆå¹¶æˆ–åˆ†è£‚ç°‡ã€‚è¿™æ„å‘³ç€åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œéƒ½ä¼šæ ¹æ®å½“å‰çš„ä¿¡æ¯é€‰æ‹©æœ€ä¼˜çš„åˆå¹¶æˆ–åˆ†è£‚æ“ä½œï¼Œè€Œä¸æ˜¯è€ƒè™‘å…¨å±€æœ€ä¼˜è§£ã€‚
   - è¿™ç§è´ªå©ªç­–ç•¥è™½ç„¶å¯èƒ½æ— æ³•æ‰¾åˆ°å…¨å±€æœ€ä¼˜çš„èšç±»ç»“æœï¼Œä½†ç”±äºå…¶è®¡ç®—æ•ˆç‡è¾ƒé«˜ï¼Œå› æ­¤åœ¨å®è·µä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚

##### 3. Time and Space requirements
<p style="display: block;">
  <img src="image_161.png" alt="image_161"/>
</p>

##### 4. Problems and Limitations
- Once a decision is made to combine two clusters, it cannot be undone
- No global objective function is directly minimized
- Different schemes have problems with one or more of the following:
  1. Sensitivity to noise
  2. Difficulty handling clusters of different sizes and non- globular shapes
  3. Breaking large clusters

##### 5. Euclidean Distance
$$
d(x,y) = \sqrt{\sum_{k=1}^{n}(x_k - y_k)^2}
$$
- where n is the number of dimensions (attributes) and xk and yk are, respectively, the kth attributes (components) or data objects x and y.

<note>Standardization is necessary if scales differ.</note>

#### 3. Other Distinctions Between Sets of Clusters
- Exclusive versus non-exclusive
  - In non-exclusive clusterings, points may belong to multiple
  clusters.
    - Can belong to multiple classes or could be â€˜borderâ€™ points
  - Fuzzy clustering	(one type of non-exclusive)
    - In fuzzy clustering, a point belongs to every cluster with some weight
        between 0 and 1
    - Weights must sum to 1
    - Probabilistic clustering has similar characteristics
- Partial versus complete
  - In some cases, we only want to cluster some data

### 4. Association Rule Discovery: Definition
- Given a set of records each of which contain
some number of items from a given collection
- Produce dependency rules which will predict
occurrence of an item based on occurrences of other
items.

1. Itemset
   - A collection of one or more items

   - k-itemset
     - An itemset that contains k items

2. Support count (Ïƒ)
   - Frequency of occurrence of an itemset

3. Support
   - Fraction of transactions that contain an itemset

4. Frequent Itemset
   - An itemset whose support is greater than or equal to a minsup threshold

5. Association Rule and Rule evaluation matrix

<p style="display: block;">
  <img src="image_139.png" alt="image_139"/>
</p>

<p style="display: block;">
  <img src="image_140.png" alt="image_140"/>
</p>

<p style="display: block;">
  <img src="image_141.png" alt="image_141"/>
</p>

- Given d items, there are 2d possible candidate itemsets
<p style="display: block;">
  <img src="image_142.png" alt="image_142"/>
</p>

1. Reduce the number of candidates (M)
   - Complete search: M=2d
   - Use pruning techniques to reduce M
   - Apriori principle:
     - If an itemset is frequent, then all of its subsets must also be frequent

   - Apriori principle holds due to the following property of the support measure:
     - Support of an itemset never exceeds the support of its subsets
     - This is known as the anti-monotone property of support
     <note>å…·ä½“ä¾‹å­ä¸æ­¥éª¤è§week5 ppt ğŸ«  å“¥ä»¬å†™åˆ°è¿™å·²ç»å¿«æ­»äº†</note>

2. Reduce the number of transactions (N)
   - Reduce size of N as the size of itemset increases
3. Reduce the number of comparisons (NM)
   - Use efficient data structures to store the candidates or transactions
   - No need to match every candidate against every transaction

#### Apriori Algorithm
<p style="display: block;">
  <img src="image_143.png" alt="image_143"/>
</p>

<p style="display: block;">
  <img src="image_145.png" alt="image_145"/>
</p>

<p style="display: block;">
  <img src="image_144.png" alt="image_144"/>
</p>

<p style="display: block;">
  <img src="image_146.png" alt="image_146"/>
</p>

<p style="display: block;">
  <img src="image_147.png" alt="image_147"/>
</p>

<p style="display: block;">
  <img src="image_148.png" alt="image_148"/>
</p>

<p style="display: block;">
  <img src="image_149.png" alt="image_149"/>
</p>

<p style="display: block;">
  <img src="image_150.png" alt="image_150"/>
</p>

<p style="display: block;">
  <img src="image_151.png" alt="image_151"/>
</p>

- Factors Affecting Complexity of Apriori
  - Choice of minimum support threshold
    - lowering support threshold results in more frequent itemsets
    - this may increase number of candidates and max length of frequent itemsets
  - Dimensionality (number of items) of the data set
    - More space is needed to store support count of	itemsets
    - if number of frequent itemsets also increases, both computation and I/O costs may also increase
  - Size of database
    - run time of algorithm increases with number of transactions
  - Average transaction width
    - transaction width increases the max length of frequent itemsets
    - number of subsets in a transaction increases with its width, increasing computation time for support counting


#### 1. Association Analysis: Applications

- Market-basket analysis 
  - Rules are used for sales promotion, shelf management, and inventory management

- Telecommunication alarm diagnosis 
  - Rules are used to find combination of alarms that  occur together frequently in the same time period

- Medical Informatics 
  - Rules are used to find combination of patient  symptoms and test results associated with certain  diseases

## 4. Data
- **Data:** Collection of data objects and their attributes
- **Attribute Values:** Attribute values are numbers or symbols assigned
  to an attribute
  - Distinction between attributes and attribute values
    - Same attribute can be mapped to different attribute
      values
    - Different attributes can be mapped to the same set of
      values

### 1. Types of Attributes

| Property       | Symbol | Nominal | Ordinal | Interval | Ratio |
|----------------|--------|---------|---------|----------|-------|
| Distinctness   | = â‰     | âœ…       | âœ…       | âœ…        | âœ…     |
| Order          | < >    |         | âœ…       | âœ…        | âœ…ï¸    |
| Addition       | + -    |         |         | âœ…        | âœ…ï¸    |
| Multiplication | * /    |         |         |          | âœ…     |

| Attribute Type  | Description                                                                | Examples                                                                       | Operations                                                     | Transformation                                                                                      | Comments                                                                                                                               |
|-----------------|----------------------------------------------------------------------------|--------------------------------------------------------------------------------|----------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|
| **Categorical** |                                                                            |                                                                                |                                                                |                                                                                                     |                                                                                                                                        |
| Nominal         | Nominal attribute values only distinguish. (=, â‰ )                          | zip codes, employee ID numbers, eye color, sex: {male, female}                 | mode, entropy, contingency correlation, Ï‡Â² test                | Any permutation of values                                                                           | If all employee ID numbers were reassigned, would it make any difference?                                                              |
| Ordinal         | Ordinal attribute values also order objects. (<, >)                        | hardness of minerals, {good, better, best}, grades, street numbers             | median, percentiles, rank correlation, run tests, sign tests   | An order preserving change of values, i.e.,new_value = f(old_value) where f is a monotonic function | An attribute encompassing the notion of good, better, best can be represented equally well by the values {1, 2, 3} or by {0.5, 1, 10}. |
| **Numeric**     |                                                                            |                                                                                |                                                                |                                                                                                     |                                                                                                                                        |
| Interval        | For interval attributes, differences between values are meaningful. (+, -) | calendar dates, temperature in Celsius or Fahrenheit                           | mean, standard deviation, Pearson's correlation, t and F tests | new_value = a * old_value + b where a and b are constants                                           | Thus, the Fahrenheit and Celsius temperature scales differ in terms of where their zero value is and the size of a unit (degree).      |
| Ratio           | For ratio variables, both differences and ratios are meaningful. (*, /)    | temperature in Kelvin, monetary quantities, counts, age, mass, length, current | geometric mean, harmonic mean, percent variation               | new_value = a * old_value                                                                           | Length can be measured in meters or feet.                                                                                              |

### 2. Discrete and Continuous Attributes

#### Discrete Attribute ç¦»æ•£å±æ€§
- Has only a finite or countably infinite set of values
- Examples: zip codes, counts, or the set of words in a collection of documents
- Often represented as integer variables
- Note: binary attributes are a special case of discrete attributes

#### Continuous Attribute è¿ç»­å±æ€§
- Has real numbers as attribute values
- Examples: temperature, height, or weight
- Practically, real values can only be measured and represented using a finite number of digits
- Continuous attributes are typically represented as floating-point variables

### 3. Asymmetric Attributes
- Only presence (a non-zero attribute value) is regarded as
  important.

### 4. Critiques of the Attribute Categorization

#### Incomplete
- Asymmetric binary
- Cyclical (e.g., position on the surface of the Earth, Time)
- Multivariate (e.g., set of movies seen)
- Partially ordered
- Partial membership
- Relationships between the data

#### Real Data is Approximate and Noisy
- This can complicate recognition of the proper attribute type.
- Treating one attribute type as another may be approximately correct.

### 5. Key Messages for Attribute Types

#### Meaningful Operations
- The types of operations you choose should be "meaningful" for the type of data you have.
  - Distinctness, order, meaningful intervals, and meaningful ratios are only four (among many possible) properties of data.
  - The data type you seeâ€”often numbers or stringsâ€”may not capture all the properties or may suggest properties that are not present.
  - Analysis may depend on these other properties of the data.
    - Many statistical analyses depend only on the distribution.
  - In the end, what is meaningful can be specific to the domain.

### 6. Important Characteristics of Data

- **Dimensionality (number of attributes)**
  - High dimensional data brings a number of challenges

- **Sparsity**
  - Only presence counts

- **Resolution**
  - Patterns depend on the scale

- **Size**
  - Type of analysis may depend on size of data

### 7. Types of Data Sets

<tip>è¯¦æƒ…è§ Data_Mining_week2_slides.pdf</tip>

#### Record
- **Data Matrix**
- **Document Data**
- **Transaction Data**

#### Graph
- **World Wide Web**
- **Molecular Structures**

#### Ordered
- **Spatial Data**
- **Temporal Data**
- **Sequential Data**
- **Genetic Sequence Data**

### 8. Data Quality
- Poor data quality negatively affects many data processing
  efforts
- Quality issues include:
  - Noise
    - For objects, noise is an extraneous (ç„¡é—œ) object
    - For attributes, noise refers to modification of original values
  - Outliers
    - Outliers are data objects with characteristics that
      are considerably different from most of the other
      data objects in the data set
  - Wrong data
  - Fake data
  - Missing values
    - Reasons for missing values
      - Information is not collected
      - Attributes may not be applicable to all cases
    -  Handling missing values
      - Eliminate data objects or variables
      - Estimate missing values
      - Ignore the missing value during analysis
  - Duplicate data
    - Data set may include data objects that are
      duplicates, or almost duplicates of one another

### 9. Data Preprocessing
#### 1. Aggregation
- Combining two or more attributes (or objects) into a single
  attribute (or object)
- Purpose
  - Data reduction - reduce the number of attributes or objects
  - Change of scale
    - Cities aggregated into regions, states, countries, etc.
    - Days aggregated into weeks, months, or years
  - More â€œstableâ€ data - aggregated data tends to have less variability
#### 2. Sampling
- Sampling is the main technique used for data
  reduction.
  - It is often used for both the preliminary investigation of
    the data and the final data analysis.
- Statisticians often sample because getting the
  entire set of data of interest is too expensive or
  time consuming.
- Sampling is typically used in data mining because
  processing the entire set of data of interest is too
  expensive or time consuming.
- The **key principle** for effective sampling is the
  following:
  - Using a sample will work almost as well as using the
    entire data set, if the sample is representative
  - A sample is representative if it has approximately the
    same properties (of interest) as the original set of data

  - **Simple Random Sampling**
  - There is an equal probability of selecting any particular item.

    - **Sampling without replacement**
    - As each item is selected, it is removed from the population.

    - **Sampling with replacement**
    - Objects are not removed from the population as they are selected for the sample.
    - In sampling with replacement, the same object can be picked up more than once.

  - **Stratified sampling**
  - Split the data into several partitions; then draw random samples from each partition.


#### 3. Discretization
- Discretization is the process of converting a
  continuous attribute into an ordinal attribute
  - A potentially infinite number of values are mapped into
    a small number of categories
  - Discretization is used in both unsupervised and
    supervised settings
- Unsupervised Discretization
  - Equal interval width (distance) partitioning
  - Equal-frequency (frequency) partitioning
  - K-means
- Supervised Discretization
  - Discretization is based on class labels
  - The goal is to find partitions that minimize the
    entropy (or maximize the purity) of the classes in
    each partition

#### 4. Binarization
- Binarization maps a continuous or categorical
  attribute into one or more binary variables

#### 5. Attribute Transformation
- An attribute transform is a function that maps the
  entire set of values of a given attribute to a new
  set of replacement values such that each old
  value can be identified with one of the new values

- Simple Functions

  - Power Function (å¹‚å‡½æ•°):
    $x^k$
  - Logarithmic Function (å¯¹æ•°å‡½æ•°):
    $log(x)$
  - Exponential Function (æŒ‡æ•°å‡½æ•°):
    $ e^x $
  - Absolute Value Function (ç»å¯¹å€¼å‡½æ•°):
    $|x|$

- Normalization
  - Refers to various techniques to adjust to
    differences among attributes in terms of frequency
    of occurrence, mean, variance, range
  - Take out unwanted, common signal, e.g.,
    seasonality
- In statistics, standardization refers to subtracting off
  the means and dividing by the standard deviation

#### 5. Dimensionality Reduction

- Curse of Dimensionality
  - Many types of data analysis become significantly harder as
    the dimensionality of the data increases
  - When dimensionality increases, data becomes increasingly
    sparse in the space that it occupies
  - Definitions of density and distance between points, which
    are critical for clustering and outlier detection, become less
    meaningful

- Purpose of Dimensionality Reduction:
  - Avoid curse of dimensionality
  - Reduce amount of time and memory required by data
    mining algorithms
  - Allow data to be more easily visualized
  - May help to eliminate irrelevant features or reduce
    noise
- Techniques
  - Principal Components Analysis (PCA)
  - Singular Value Decomposition
  - Others: supervised and non-linear techniques

#### 6. Feature subset selection
- Another way to reduce dimensionality of data
- Redundant features
  - Duplicate much or all of the information contained in
    one or more other attributes
  - Example: purchase price of a product and the amount
    of sales tax paid
- Irrelevant features
  - Contain no information that is useful for the data
    mining task at hand
  - Example: students' ID is often irrelevant to the task of
    predicting students' GPA
- Many techniques developed, especially for
  classification

#### 7. Feature creation
- Create new attributes that can capture the
  important information in a data set much more
  efficiently than the original attributes
- Three general methodologies:
  - Feature extraction
  - Feature construction
  - Mapping data to new space

## 5. Decision Trees

<note>æœ¬ç« å…·ä½“å†…å®¹è§ä»¥ä¸‹é“¾æ¥</note>

[](#1-decision-tree-algorithms)

## 6. Model Evaluation

### 1. Metrics for Performance Evaluation
<tip>IMPORTANT!!!</tip>

![image_9.png](image_9.png)

### Confusion Matrix æ··æ·†çŸ©é˜µ
|                 | Predicted Positive  | Predicted Negative  |
|-----------------|---------------------|---------------------|
| Actual Positive | True Positive (TP)  | False Negative (FN) |
| Actual Negative | False Positive (FP) | True Negative (TN)  |

#### 1. Accuracy å‡†ç¡®ç‡
Accuracyè¡¨ç¤ºåˆ†ç±»å™¨æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬æ¯”ä¾‹ã€‚

$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$

#### 2. Precision ç²¾ç¡®ç‡
Precisionè¡¨ç¤ºè¢«é¢„æµ‹ä¸ºæ­£ç±»çš„æ ·æœ¬ä¸­å®é™…ä¸ºæ­£ç±»çš„æ¯”ä¾‹ã€‚

$
\text{Precision} = \frac{TP}{TP + FP}
$

#### 3. Recall (Sensitivity) å¬å›ç‡ï¼ˆçµæ•åº¦ï¼‰
Recallè¡¨ç¤ºå®é™…ä¸ºæ­£ç±»çš„æ ·æœ¬ä¸­è¢«æ­£ç¡®é¢„æµ‹ä¸ºæ­£ç±»çš„æ¯”ä¾‹ã€‚

$
\text{Recall} = \frac{TP}{TP + FN}
$

#### 4. F-measure (F1-score)
F-measureæ˜¯Precisionå’ŒRecallçš„è°ƒå’Œå¹³å‡å€¼ã€‚

$
\text{F-measure} = 2 \times\frac{ \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$

#### 5. Specificity ç‰¹å¼‚æ€§
Specificityè¡¨ç¤ºå®é™…ä¸ºè´Ÿç±»çš„æ ·æœ¬ä¸­è¢«æ­£ç¡®é¢„æµ‹ä¸ºè´Ÿç±»çš„æ¯”ä¾‹ã€‚

$
\text{Specificity} = \frac{TN}{TN + FP}
$

#### 6. False Positive Rate (FPR) å‡é˜³æ€§ç‡
FPRè¡¨ç¤ºå®é™…ä¸ºè´Ÿç±»çš„æ ·æœ¬ä¸­è¢«é”™è¯¯é¢„æµ‹ä¸ºæ­£ç±»çš„æ¯”ä¾‹ã€‚

$
\text{False Positive Rate} = \frac{FP}{TN + FP}
$

### 2. Limitation of Accuracy

- Consider a 2-class problem
  - Number of Class 0 examples = 9990
  - Number of Class 1 examples = 10
-  If model predicts everything to be class 0,
accuracy is 9990/10000 = 99.9 %
  - Accuracy is misleading because model does
  not detect any class 1 example

### 3. Computing Cost of Classification
Cost Matrix
<p style="display: block;">
  <img src="image_133.png" alt="image_133"/>
</p>

<p style="display: block;">
  <img src="image_135.png" alt="image_135"/>
</p>

<p style="display: block;">
  <img src="image_134.png" alt="image_134"/>
</p>

<p style="display: block;">
  <img src="image_136.png" alt="image_136"/>
</p>

### 4. Methods for Performance Evaluation
Performance of a model may depend on other
factors besides the learning algorithm:
- Class distribution
- Cost of misclassification
- Size of training and test sets

### 5. Learning Curve
- Learning curve shows
  how accuracy changes
  with varying sample size
- Requires a sampling
  schedule for creating
  learning curve
- Effect of small sample
  size:
  - Bias in the estimate
  - Variance of estimate

### 6. Methods of Estimation
- Holdout
  - Reserve k% for training and (100-k)% for testing
- Random subsampling
  - Repeated holdout
- Cross validation
  - Partition data into k disjoint subsets
  - k-fold: train on k-1 partitions, test on the remaining one
  - Leave-one-out: k=n
- Stratified (åˆ†å±¤) sampling
  - oversampling vs undersampling
- Bootstrap
  - Sampling with replacement

#### Variations on Cross-validation
- Repeated cross-validation
  - Perform cross-validation a number of times
  - Gives an estimate of the variance of the
  generalization error
- Stratified cross-validation
  - Guarantee the same percentage of class
  labels in training and test
  - Important when classes are imbalanced and
  the sample is small
- Use nested cross-validation approach for model
  selection and evaluation

### 7. Methods for Model Comparison
ROC (Receiver Operating Characteristic) Curve

- ROC curve plots TPR (on the y-axis) against FPR
(on the x-axis)
  - TPR (TP rate) = TP/(TP+FN); positive hits
  - FPR (FP rate)= FP/(FP + TN); false alarms
<p style="display: block;">
  <img src="image_137.png" alt="image_137"/>
</p>
<p style="display: block;">
  <img src="image_138.png" alt="image_138"/>
</p>
<note>AUCï¼ˆæ›²çº¿ä¸‹é¢ç§¯ï¼‰ç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚AUCå€¼åœ¨0åˆ°1ä¹‹é—´ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºæ¨¡å‹æ€§èƒ½è¶Šå¥½ã€‚</note>

## 7. Classification

- Goal: previously unseen records should be
  assigned a class as accurately as possible.

- General Approach for Building
  Classification Model

<p style="display: block;">
  <img src="image_118.png" alt="image_118"/>
</p>

### Classification Techniques
- Base Classifiers
  - Decision Tree based Methods
  - Rule-based Methods
  - Nearest-neighbor
  - NaÃ¯ve Bayes and Bayesian Belief Networks
  - Support Vector Machines
  - Neural Networks, Deep Neural Nets

- Ensemble Classifiers
  - Boosting
  - Bagging
  - Random Forests

<p style="display: block;">
  <img src="image_119.png" alt="image_119"/>
</p>

### 1. Decision Tree Algorithms
1. Huntâ€™s Algorithm (one of the earliest)
   - General Structure of Huntâ€™s Algorithm
   - Let Dt be the set of training
     records that reach a node t
     - General Procedure:
       - If Dt contains records that
       belong the same class yt,
       then t is a leaf node
       labeled as yt
       - If Dt contains records that
       belong to more than one
       class, use an attribute test
       to split the data into smaller
       subsets. Recursively apply
       the procedure to each
       subset.
2. CART
3. ID3, C4.5
4. SLIQ, SPRINT

### 2. Design Issues of Decision Tree Induction

1. Greedy strategy
   - Split the records based on an attribute test that
   optimizes certain criterion
2. How should training records be split?
   - Method for expressing test condition
     - depending on attribute types
   - Measure for evaluating the goodness of a test
     condition
3. How should the splitting procedure stop?
   - Stop splitting if all the records belong to the same
   class or have identical attribute values
   - Early termination

### 3. Methods for Expressing Test Conditions

1. Binary 
2. Nominal 
   - Multi-way split:
     - Use as many partitions as distinct values.
   
   - Binary split:
     - Divides values into two subsets

   - ![image_6.png](image_6.png)
   
3. Ordinal 
   - Multi-way split:
     - Use as many partitions as distinct values

   - Binary split:
     - Divides values into two subsets
     - Preserve order property among attribute values

   - ![image_7.png](image_7.png)
   
4. Continuous
   - ![image_8.png](image_8.png)
   - Splitting Based on Continuous Attributes
     - Discretization to form an ordinal categorical
       attribute
     - Ranges can be found by equal interval bucketing,
       equal frequency bucketing (percentiles), or
       clustering.
       - Static â€“ discretize once at the beginning
       - Dynamic â€“ repeat at each node
   - Binary Decision: (A < v) or (A >= v)
     - consider all possible splits and finds the best cut
     - can be more compute intensive

### 4. How to determine the Best Split

- Greedy approach:
  - Nodes with purer class distribution are preferred

### 5. Measures of Node Impurity
<tip>åº”è¯¥é‡è¦å§</tip>

- $p_i(t)$ æ˜¯èŠ‚ç‚¹ $t$ ä¸Šç±» $i$ çš„é¢‘ç‡ï¼Œ $c$ æ˜¯ç±»çš„æ€»æ•°ã€‚

#### 1. Gini Index

$$
Gini \ Index = 1 - \sum_{i=0}^{c-1} p_i(t)^2
$$

For 2-class problem (p, 1 â€“ p):

$$
GINI = 1 â€“ p2 â€“ (1 â€“ p)2 = 2p (1-p)
$$

- When a node $p$ is split into $k$ partitions (children)

$$
GINI_{split} = \sum_{i=1}^{k} \frac{n_i}{n} GINI(i)
$$
where,
$$
n_i = \text{number of records at child } i
$$
$$
n = \text{number of records at parent node } p
$$

- Binary Attributes: Computing GINI Index
<p style="display: block;">
  <img src="image_120.png" alt="image_120"/>
</p>
<p style="display: block;">
  <img src="image_122.png" alt="image_122"/>
</p>

- Categorical Attributes: Computing Gini Index
<p style="display: block;">
  <img src="image_121.png" alt="image_121"/>
</p>

- Continuous Attributes: Computing Gini Index
<p style="display: block;">
  <img src="image_123.png" alt="image_123"/>
</p>

<p style="display: block;">
  <img src="image_124.png" alt="image_124"/>
</p>

<tip>Giniåæ˜ çš„æ˜¯ä¸çº¯åº¦ï¼ˆimpurityï¼‰ï¼Œæ‰€ä»¥è¶Šä½è¶Šå¥½</tip>
<tip>Gainç®—çš„æ˜¯å¢ç›Šï¼Œé«˜çš„å¥½</tip>

#### 2. Entropy (ç†µ)

$$
Entropy = -\sum_{i=0}^{c-1} p_i(t) \log_2 p_i(t)
$$

- Computing Entropy of a Single Node
<p style="display: block;">
  <img src="image_125.png" alt="image_125"/>
</p>

- Computing Information Gain After Splitting

  - $$
  Gain_{split} = Entropy(p) - \sum_{i=1}^{k} \frac{n_i}{n} Entropy(i)
  $$

  - Parent Node, $p$ is split into $k$ partitions (children)  
  $n_i$ is the number of records in child node $i$

  - Choose the split that achieves the most reduction (maximizes GAIN)
  - Used in the ID3 and C4.5 decision tree algorithms
  - Information gain is the mutual information between the class variable and the splitting variable

#### 3. Problem with large number of partitions
- **Node impurity measures tend to prefer splits that
  result in large number of partitions, each being
  small but pure**
- Customer ID has highest information gain
  because entropy for all the children is zero

#### 4. Gain Ratio

- Gain Ratio:
$$
\text{Gain Ratio} = \frac{\text{Gain}_{\text{split}}}{\text{Split Info}}
$$

- Split Info Formula:
$$
\text{Split Info} = - \sum_{i=1}^{k} \frac{n_i}{n} \log_2 \frac{n_i}{n}
$$

- Definitions:
- **Parent Node**, $p$ is split into $k$ partitions (children)
- $n_i$ is the number of records in child node $i$

- Key Points:
- Adjusts Information Gain by the entropy of the partitioning (\( \text{Split Info} \)).
    - Higher entropy partitioning (large number of small partitions) is penalized!
- Used in C4.5 algorithm
- Designed to overcome the disadvantage of Information Gain

#### 5. Misclassification Error (é”™è¯¯åˆ†ç±»ç‡)

$$
Classification \ error = 1 - \max[p_i(t)]
$$

- Computing Error of a Single Node
<p style="display: block;">
  <img src="image_126.png" alt="image_126"/>
</p>

### 6. Finding the Best Split
1. Compute impurity measure (P) before splitting 
2. Compute impurity measure (M) after splitting
   - Compute impurity measure of each child node
   - M is the weighted impurity of child nodes
3. Choose the attribute test condition that produces the highest gain

$$ Gain = P - M $$

or equivalently, lowest impurity (highest purity) measure after splitting (M)

### 7. Comparison among Impurity Measure
<p style="display: block;">
  <img src="image_127.png" alt="image_127"/>
</p>

### 8. Decision Tree Based Classification
- **Advantages:**
    - Relatively inexpensive to construct
    - Extremely fast at classifying unknown records
    - Easy to interpret for small-sized trees
    - Robust to noise (especially when methods to avoid overfitting are used)
    - Can easily handle redundant attributes
    - Can easily handle irrelevant attributes (unless the attributes are interacting)

- **Disadvantages:**
    - Due to the greedy nature of splitting criterion, interacting attributes (that can distinguish between classes together but not individually) may be passed over in favor of other attributes that are less discriminating.
    - Each decision boundary involves only a single attribute

#### 1. Data Fragmentation
- Number of instances gets smaller as you traverse
  down the tree
- Number of instances at the leaf nodes could be
too small to make any statistically significant
decision

#### 2. Search Strategy 
- Finding an optimal decision tree is NP-hard
- The algorithm presented so far uses a greedy,
  top-down, recursive partitioning strategy to
  induce a reasonable solution
- Other strategies:
    - Bottom-up
    - Bi-directional

#### 3. Expressiveness

- Decision trees provide expressive representation for learning discrete-valued functions.

- **Do not generalize well to certain types of Boolean functions.**
    - **Example: Parity Function**
        - Class = 1 if there is an even number of Boolean attributes with truth value = True.
        - Class = 0 if there is an odd number of Boolean attributes with truth value = True.
    - For accurate modeling, must have a complete tree.

- **Not expressive enough for modeling continuous variables.**
    - Particularly when test condition involves only a single attribute at-a-time.

##### Decision Boundary
- Border line between two neighboring regions of different classes is
  known as decision boundary
- Decision boundary is parallel to axes because test condition involves
  a single attribute at-a-time
<p style="display: block;">
  <img src="image_128.png" alt="image_128"/>
</p>

##### Oblique Decision Trees
<p style="display: block;">
  <img src="image_129.png" alt="image_129"/>
</p>

#### 4. Tree Replication
<p style="display: block;">
  <img src="image_130.png" alt="image_130"/>
</p>

### 9. Practical Issues of Classification

- **Classification Errors**
1. Training errors: Errors committed on the training set
2. Test errors: Errors committed on the test set
3. Generalization errors: Expected error of a model over random selection of records from same distribution

#### 1. Underfitting and Overfitting
- Underfitting: when model is too simple, both training and test errors are large
- Overfitting: when model is too complex, training error is small but test error is large
  - Increasing the size of training data reduces the difference between training and
    testing errors at a given size of model
  - Reasons for Model Overfitting
    - Noisy data
    - Not enough training data
    - High model complexity
<note>
Overfitting results in decision trees that are more
complex than necessary 
</note>
<note>
Training error does not provide a good estimate
of how well the tree will perform on previously
unseen records
</note>
<note>
Need ways for estimating generalization errors
</note>

#### 2. Missing Values

- Missing values affect decision tree construction in
three different ways:
  - Affects how impurity measures are computed
  - Affects how to distribute instance with missing
  value to child nodes
  - Affects how a test instance with missing value
  is classified
  
<p style="display: block;">
  <img src="image_132.png" alt="image_132"/>
</p>

#### 3.  Model Evaluation/Costs of Classification

[](#6-model-evaluation)

### 10. Model selection
- Performed during model building
- Purpose is to ensure that model is not overly
complex (to avoid overfitting)
- Need to estimate generalization error
  - Using Validation Set
  - Divide training data into two parts:
    - Training set:
      - use for model building
    - Validation set:
      - use for estimating generalization error
      - Note: validation set is not the same as test set
    - Drawback:
      - Less data available for training
  - Incorporating Model Complexity
    - Rationale: Occamâ€™s Razor
      - Given two models of similar generalization errors,
      one should prefer the simpler model over the more
      complex model
      - A complex model has a greater chance of being fitted
      accidentally
      - Therefore, one should include model complexity when
      evaluating a model
      - Generalization Error (æ³›åŒ–è¯¯å·®)
      - $$
      \text{Gen. Error(Model)} = \text{Train. Error(Model, Train. Data)} + \alpha \times \text{Complexity(Model)}
      $$
    
#### 1. Estimating the Complexity of Decision Trees
##### Pessimistic Error Estimate of Decision Tree T with k Leaf Nodes

Pessimistic: it assumes that the generalization error will be larger than the training error, so it is necessary to add the penalty term

$$
err_{gen}(T) = err(T) + \Omega \times \frac{k}{N_{train}}
$$

- **$err(T)$**: error rate on all training records
- **$\Omega$**: trade-off hyper-parameter (similar to \alpha)
    - Relative cost of adding a leaf node
- **$k$**: number of leaf nodes
- **$N_{train}$**: total number of training records

Example:
<p style="display: block;">
  <img src="image_131.png" alt="image_131"/>
</p>

- Resubstitution Estimate:
- Using training error as an optimistic estimate of
generalization error
- Referred to as optimistic error estimate

#### 2. Pre-Pruning (Early Stopping Rule)

- **Stop the algorithm before it becomes a fully-grown tree**
- **Typical stopping conditions for a node:**
    - Stop if all instances belong to the same class
    - Stop if all the attribute values are the same
- **More restrictive conditions:**
    - Stop if number of instances is less than some user-specified threshold
    - Stop if class distribution of instances are independent of the available features (e.g., using $\chi^2$ test)
    - Stop if expanding the current node does not improve impurity measures (e.g., Gini or information gain)
    - Stop if estimated generalization error falls below certain threshold

#### 3. Post-Pruning

- **Grow decision tree to its entirety**
- **Subtree replacement**
    - Trim the nodes of the decision tree in a bottom-up fashion
    - If generalization error improves after trimming, replace sub-tree by a leaf node
    - Class label of leaf node is determined from majority class of instances in the sub-tree

## 8. Ensemble Methods
- Construct a set of base classifiers learned from the training data
- Predict class label of test records by combining the predictions made by multiple classifiers (e.g., by taking majority vote)




      